"","title","author","subject","abstract","meta"
"1","HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers","Francesco Dibitonto, Fabio Garcea, André Panisson, Alan Perotti, Lia Morra","Computer Vision and Pattern Recognition (cs.CV)","Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units. Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach. On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence. The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves. All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it. The code is available at this https URL.","Wed, 13 Mar 2024 13:51:02 UTC (26,380 KB)"
"2","Pig aggression classification using CNN, Transformers and Recurrent Networks","Junior Silva Souza, Eduardo Bedin, Gabriel Toshio Hirokawa Higa, Newton Loebens, Hemerson Pistori","Computer Vision and Pattern Recognition (cs.CV)","The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques. The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors. In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification. The performance was evaluated using accuracy, precision, and recall. The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729.","Wed, 13 Mar 2024 13:38:58 UTC (11,743 KB)"
"3","A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product","Ao Xiang, Zongqing Qi, Han Wang, Qin Yang, Danqing Ma","Computer Vision and Pattern Recognition (cs.CV)","In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems. The dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements. improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.","Wed, 13 Mar 2024 13:16:26 UTC (501 KB)"
"4","Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks","Zongqing Qi, Danqing Ma, Jingyu Xu, Ao Xiang, Hedi Qu","Computer Vision and Pattern Recognition (cs.CV)","In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems.The dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements.Improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.","Wed, 13 Mar 2024 13:07:14 UTC (774 KB)"
"5","Interactive environments for training children's curiosity through the practice of metacognitive skills: a pilot study","Rania Abdelghani, Edith Law, Chloé Desvaux, Pierre-Yves Oudeyer, Hélène Sauzéon","Computers and Society (cs.CY)","Curiosity-driven learning has shown significant positive effects on students' learning experiences and outcomes. But despite this importance, reports show that children lack this skill, especially in formal educational settings. To address this challenge, we propose an 8-session workshop that aims to enhance children's curiosity through training a set of specific metacognitive skills we hypothesize are involved in its process. Our workshop contains animated videos presenting declarative knowledge about curiosity and the said metacognitive skills as well as practice sessions to apply these skills during a reading-comprehension task, using a web platform designed for this study (e.g. expressing uncertainty, formulating questions, etc). We conduct a pilot study with 15 primary school students, aged between 8 and 10. Our first results show a positive impact on children's metacognitive efficiency and their ability to express their curiosity through question-asking behaviors.","Wed, 13 Mar 2024 10:21:32 UTC (2,152 KB)"
"6","Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts","Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen","Computer Vision and Pattern Recognition (cs.CV)","Despite recent advances in image-to-video generation, better controllability and local animation are less explored. Most existing image-to-video methods are not locally aware and tend to move the entire scene. However, human artists may need to control the movement of different objects or regions. Additionally, current I2V methods require users not only to describe the target motion but also to provide redundant detailed descriptions of frame contents. These two issues hinder the practical utilization of current I2V tools. In this paper, we propose a practical framework, named Follow-Your-Click, to achieve image animation with a simple user click (for specifying what to move) and a short motion prompt (for specifying how to move). Technically, we propose the first-frame masking strategy, which significantly improves the video generation quality, and a motion-augmented module equipped with a short motion prompt dataset to improve the short prompt following abilities of our model. To further control the motion speed, we propose flow-based motion magnitude control to control the speed of target movement more precisely. Our framework has simpler yet precise user control and better generation performance than previous methods. Extensive experiments compared with 7 baselines, including both commercial tools and research methods on 8 metrics, suggest the superiority of our approach. Project Page: this https URL","Wed, 13 Mar 2024 05:44:37 UTC (37,358 KB)"
"7","Matching Non-Identical Objects","Yusuke Marumo, Kazuhiko Kawamoto, Hiroshi Kera","Computer Vision and Pattern Recognition (cs.CV)","Not identical but similar objects are everywhere in the world. Examples include four-legged animals such as dogs and cats, cars of different models, akin flowers in various colors, and countless others. In this study, we address a novel task of matching such non-identical objects. We propose a simple weighting scheme of descriptors that enhance various sparse image matching methods, which are originally designed for matching identical objects captured from different perspectives, and achieve semantically robust matching. The experiments show successful matching between non-identical objects in various cases including domain shift. Further, we present a first evaluation of the robustness of the image matching methods under common corruptions, which is a sort of domain shift, and the proposed method improves the matching in this case as well.","Wed, 13 Mar 2024 04:11:38 UTC (11,531 KB)"
"8","Progressive and rushed Dyck paths","Axel Bacher","Combinatorics (math.CO)","We call progressive paths and rushed paths two families of Dyck paths studied by Asinowski and Jelínek, which have the same enumerating sequence (OEIS entry A287709). We present a bijection proving this fact. Rushed paths turn out to be in bijection with \emph{one-sided trees}, introduced by Durhuus and Ünel, which have an asymptotic enumeration involving a stretched exponential. We conclude by presenting several other classes of related lattice paths and directed animals that may have similar asymptotic properties.","Tue, 12 Mar 2024 23:13:21 UTC (10 KB)"
"9","AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production","Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao, Xin Tu, Zhenyu Guo","Computer Vision and Pattern Recognition (cs.CV)","The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for multimodal content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these multimodal contents into videos. Additionally, the animating units (e.g., Gen-2 and Sora) could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary system that optimizes the whole video generation workflow and the steps within the workflow. It continuously evolves and iteratively optimizes workflow by accumulating expert experience and professional knowledge, including optimizing the LLM prompts and utilities usage. The Utility Layer provides multiple utilities, leading to consistent image generation that is visually coherent in terms of composition, characters, and style. Meanwhile, it provides audio and special effects, integrating them into expressive and logically arranged videos. Overall, our AesopAgent achieves state-of-the-art performance compared with many previous works in visual storytelling. Our AesopAgent is designed for convenient service for individual users, which is available on the following page: this https URL.","Tue, 12 Mar 2024 02:30:50 UTC (17,477 KB)"
"10","Formalizing Feint Actions, and Example Studies in Two-Player Games","Junyu Liu, Wangkai Jin, Xiangjun Peng","Computer Science and Game Theory (cs.GT)","Feint actions refer to a set of deceptive actions, which enable players to obtain temporal advantages from their opponents. Such actions are regarded as widely-used tactic in most non-deterministic Two-player Games (e.g. boxing and fencing). However, existing literature does not provide comprehensive and concrete formalization on Feint actions, and their implications on Two-Player Games. We argue that a full exploration on Feint actions is of great importance towards more realistic Two-player Games. In this paper, we provide the first comprehensive and concrete formalization of Feint actions. The key idea of our work is to (1) allow automatic generation of Feint actions, via our proposed Palindrome-directed Generation of Feint actions; and (2) provide concrete principles to properly combine Feint and attack actions. Based on our formalization of Feint actions, we also explore the implications on the game strategy model, and provide optimizations to better incorporate Feint actions. Our experimental results shows that accounting for Feint actions in Non-Deterministic Games (1) brings overall benefits to the game design; and (2) has great benefits on on either game animations or strategy designs, which also introduces a great extent of randomness into randomness-demanded Game models.","Sun, 3 Mar 2024 18:23:18 UTC (11,856 KB)"
"11","Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution","Alexandre de Oliveira Bezerra, Rodrigo Goncalves Mateus, Vanessa Ap. de Moraes Weber, Fabricio de Lima Weber, Yasmin Alves de Arruda, Rodrigo da Costa Gomes, Gabriel Toshio Hirokawa Higa, Hemerson Pistori","Image and Video Processing (eess.IV)","Assessing the biotype of cattle through human visual inspection is a very common and important practice in precision cattle breeding. This paper presents the results of a correlation analysis between scores produced by humans for Nelore cattle and a variety of measurements that can be derived from images or other instruments. It also presents a study using the k-means algorithm to generate new ways of clustering a batch of cattle using the measurements that most correlate with the animal's body weight and visual scores.","Mon, 11 Mar 2024 20:07:05 UTC (10,106 KB)"
"12","Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style","Shuai Tan, Bin Ji, Ye Pan","Computer Vision and Pattern Recognition (cs.CV)","Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style.","Mon, 11 Mar 2024 01:32:29 UTC (18,703 KB)[v2] Tue, 12 Mar 2024 03:12:29 UTC (18,690 KB)"
"13","Fish-inspired tracking of underwater turbulent plumes","Peter Gunnarson, John O. Dabiri","Fluid Dynamics (physics.flu-dyn)","Autonomous ocean-exploring vehicles have begun to take advantage of onboard sensor measurements of water properties such as salinity and temperature to locate oceanic features in real time. Such targeted sampling strategies enable more rapid study of ocean environments by actively steering towards areas of high scientific value. Inspired by the ability of aquatic animals to navigate via flow sensing, this work investigates hydrodynamic cues for accomplishing targeted sampling using a palm-sized robotic swimmer. As proof-of-concept analogy for tracking hydrothermal vent plumes in the ocean, the robot is tasked with locating the center of turbulent jet flows in a 13,000-liter water tank using data from onboard pressure sensors. To learn a navigation strategy, we first implemented Reinforcement Learning (RL) on a simulated version of the robot navigating in proximity to turbulent jets. After training, the RL algorithm discovered an effective strategy for locating the jets by following transverse velocity gradients sensed by pressure sensors located on opposite sides of the robot. When implemented on the physical robot, this gradient following strategy enabled the robot to successfully locate the turbulent plumes at more than twice the rate of random searching. Additionally, we found that navigation performance improved as the distance between the pressure sensors increased, which can inform the design of distributed flow sensors in ocean robots. Our results demonstrate the effectiveness and limits of flow-based navigation for autonomously locating hydrodynamic features of interest.","Sun, 10 Mar 2024 04:24:29 UTC (8,864 KB)"
"14","Audio-Synchronized Visual Animation","Lin Zhang, Shentong Mo, Yijing Zhang, Pedro Morgado","Computer Vision and Pattern Recognition (cs.CV)","Current visual generation methods can produce high quality videos guided by texts. However, effectively controlling object dynamics remains a challenge. This work explores audio as a cue to generate temporally synchronized image animations. We introduce Audio Synchronized Visual Animation (ASVA), a task animating a static image to demonstrate motion dynamics, temporally guided by audio clips across multiple classes. To this end, we present AVSync15, a dataset curated from VGGSound with videos featuring synchronized audio visual events across 15 categories. We also present a diffusion model, AVSyncD, capable of generating dynamic animations guided by audios. Extensive evaluations validate AVSync15 as a reliable benchmark for synchronized generation and demonstrate our models superior performance. We further explore AVSyncDs potential in a variety of audio synchronized generation tasks, from generating full videos without a base image to controlling object motions with various sounds. We hope our established benchmark can open new avenues for controllable visual generation. More videos on project webpage this https URL.","Fri, 8 Mar 2024 20:17:34 UTC (10,536 KB)"
"15","A Deep Learning Method for Classification of Biophilic Artworks","Purna Kar, Jordan J. Bird, Yangang Xing, Alexander Sumich, Andrew Knight, Ahmad Lotfi, Benedict Carpenter van Barthold","Computer Vision and Pattern Recognition (cs.CV)","Biophilia is an innate love for living things and nature itself that has been associated with a positive impact on mental health and well-being. This study explores the application of deep learning methods for the classification of Biophilic artwork, in order to learn and explain the different Biophilic characteristics present in a visual representation of a painting. Using the concept of Biophilia that postulates the deep connection of human beings with nature, we use an artificially intelligent algorithm to recognise the different patterns underlying the Biophilic features in an artwork. Our proposed method uses a lower-dimensional representation of an image and a decoder model to extract salient features of the image of each Biophilic trait, such as plants, water bodies, seasons, animals, etc., based on learnt factors such as shape, texture, and illumination. The proposed classification model is capable of extracting Biophilic artwork that not only helps artists, collectors, and researchers studying to interpret and exploit the effects of mental well-being on exposure to nature-inspired visual aesthetics but also enables a methodical exploration of the study of Biophilia and Biophilic artwork for aesthetic preferences. Using the proposed algorithms, we have also created a gallery of Biophilic collections comprising famous artworks from different European and American art galleries, which will soon be published on the Vieunite@ online community.","Fri, 8 Mar 2024 15:45:07 UTC (2,530 KB)"
"16","SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting","Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang","Graphics (cs.GR)","We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.","Fri, 8 Mar 2024 06:28:09 UTC (3,858 KB)"
"17","Learning Agility Adaptation for Flight in Clutter","Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao","Robotics (cs.RO)","Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads to intelligent behaviors, such as perception awareness, which distinguish it from other approaches. By deploying the policy to hardware, we verify that these advantages can be brought to the real world.","Thu, 7 Mar 2024 15:30:54 UTC (18,256 KB)"
"18","Video-Driven Animation of Neural Head Avatars","Wolfgang Paier, Paul Hinzer, Anna Hilsmann, Peter Eisert","Computer Vision and Pattern Recognition (cs.CV)","We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input. Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process. In order to achieve person-independent animation from video input, we introduce an LSTM-based animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models. Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture. We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study.","Thu, 7 Mar 2024 10:13:48 UTC (2,295 KB)"
"19","NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors","Yannan He, Garvita Tiwari, Tolga Birdal, Jan Eric Lenssen, Gerard Pons-Moll","Computer Vision and Pattern Recognition (cs.CV)","Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.","Tue, 5 Mar 2024 17:07:29 UTC (11,179 KB)"
"20","Implicit-Explicit simulation of Mass-Spring-Charge Systems","Zhiyuan Zhang, Zhaocheng Liu, Stefanos Papanicolopulos, Kartic Subr","Graphics (cs.GR)","Point masses connected by springs, or mass-spring systems, are widely used in computer animation to approximate the behavior of deformable objects. One of the restrictions imposed by these models is that points that are not topologically constrained (linked by a spring) are unable to interact with each other explicitly. Such interactions would introduce a new dimension for artistic control and animation within the computer graphics community. Beyond graphics, such a model could be an effective proxy to use for model-based learning of complex physical systems such as molecular biology. We propose to imbue masses in a mass-spring system with electrostatic charge leading a system with internal forces between all pairs of charged points -- regardless of whether they are linked by a spring. We provide a practical and stable algorithm to simulate charged mass-spring systems over long time horizons. We demonstrate how these systems may be controlled via parameters such as guidance electric fields or external charges, thus presenting fresh opportunities for artistic authoring. Our method is especially appropriate for computer graphics applications due to its robustness at larger simulation time steps.","Tue, 5 Mar 2024 14:34:13 UTC (48,434 KB)"
"21","Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation","Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng","Computer Vision and Pattern Recognition (cs.CV)","Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: this https URL.","Tue, 5 Mar 2024 09:57:47 UTC (47,643 KB)"
"22","Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery","Akram Zaytar, Caleb Robinson, Gilles Q. Hacheme, Girmaw A. Tadesse, Rahul Dodhia, Juan M. Lavista Ferres, Lacey F. Hughey, Jared A. Stabach, Irene Amoke","Computer Vision and Pattern Recognition (cs.CV)","Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches.","Tue, 5 Mar 2024 07:44:13 UTC (185 KB)"
"23","Semantic Human Mesh Reconstruction with Textures","Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang","Computer Vision and Pattern Recognition (cs.CV)","The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.","Tue, 5 Mar 2024 00:34:05 UTC (31,240 KB)"
"24","Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity","Ying-Hsuan Wu, Jun-Wei Hsieh, Li Xin, Shin-You Teng, Yi-Kuan Hsieh, Ming-Ching Chang","Machine Learning (cs.LG)","Real-world datasets commonly exhibit noisy labels and class imbalance, such as long-tailed distributions. While previous research addresses this issue by differentiating noisy and clean samples, reliance on information from predictions based on noisy long-tailed data introduces potential errors. To overcome the limitations of prior works, we introduce an effective two-stage approach by combining soft-label refurbishing with multi-expert ensemble learning. In the first stage of robust soft label refurbishing, we acquire unbiased features through contrastive learning, making preliminary predictions using a classifier trained with a carefully designed BAlanced Noise-tolerant Cross-entropy (BANC) loss. In the second stage, our label refurbishment method is applied to obtain soft labels for multi-expert ensemble learning, providing a principled solution to the long-tail noisy label problem. Experiments conducted across multiple benchmarks validate the superiority of our approach, Label Refurbishment considering Label Rarity (LR^2), achieving remarkable accuracies of 94.19% and 77.05% on simulated noisy CIFAR-10 and CIFAR-100 long-tail datasets, as well as 77.74% and 81.40% on real-noise long-tail datasets, Food-101N and Animal-10N, surpassing existing state-of-the-art methods.","Mon, 4 Mar 2024 08:06:57 UTC (1,010 KB)"
"25","Fluid ejections in nature","Elio J. Challita, Pankaj Rohilla, M. Saad Bhamla","Quantitative Methods (q-bio.QM)","From microscopic fungi to colossal whales, fluidic ejections are a universal and intricate phenomenon in biology, serving vital functions such as animal excretion, venom spraying, prey hunting, spore dispersal, and plant guttation. This review delves into the complex fluid physics of ejections across various scales, exploring both muscle-powered active systems and passive mechanisms driven by gravity or osmosis. We introduce a framework using dimensionless numbers to delineate transitions from dripping to jetting and elucidate the governing forces. Highlighting the understudied area of complex fluid ejections, this work not only rationalizes the biophysics involved but also uncovers potential engineering applications in soft robotics, additive manufacturing, and drug delivery. By bridging biomechanics, the physics of living systems, and fluid dynamics, this review offers valuable insights into the diverse world of fluid ejections and paves the way for future bioinspired research across the spectrum of life.","Mon, 4 Mar 2024 02:12:21 UTC (23,280 KB)"
"26","Piet: Facilitating Color Authoring for Motion Graphics Video","Xinyu Shi, Yinghou Wang, Yun Wang, Jian Zhao","Human-Computer Interaction (cs.HC)","Motion graphic (MG) videos are effective and compelling for presenting complex concepts through animated visuals; and colors are important to convey desired emotions, maintain visual continuity, and signal narrative transitions. However, current video color authoring workflows are fragmented, lacking contextual previews, hindering rapid theme adjustments, and not aligning with progressive authoring flows of designers. To bridge this gap, we introduce Piet, the first tool tailored for MG video color authoring. Piet features an interactive palette to visually represent color distributions, support controllable focus levels, and enable quick theme probing via grouped color shifts. We interviewed 6 domain experts to identify the frustrations in current tools and inform the design of Piet. An in-lab user study with 13 expert designers showed that Piet effectively simplified the MG video color authoring and reduced the friction in creative color theme exploration.","Mon, 4 Mar 2024 16:41:30 UTC (16,530 KB)"
"27","An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement","Haojie Shi, Tingguang Li, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Q.-H. Meng","Robotics (cs.RO)","Learning-based methods have improved locomotion skills of quadruped robots through deep reinforcement learning. However, the sim-to-real gap and low sample efficiency still limit the skill transfer. To address this issue, we propose an efficient model-based learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to directly supervise a Variational Autoencoder (VAE)-based policy network to imitate real animal behaviors. This significantly reduces the need for real interaction data and allows for rapid policy updates. We also develop a high-level network to track diverse commands and trajectories. Our simulated results show a tenfold sample efficiency increase compared to reinforcement learning methods such as PPO. In real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period and generalizes well to new speeds and paths.","Mon, 4 Mar 2024 12:01:11 UTC (16,719 KB)"
"28","FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio","Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun","Computer Vision and Pattern Recognition (cs.CV)","In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at this https URL.","Mon, 4 Mar 2024 09:59:48 UTC (2,912 KB)"
"29","APISR: Anime Production Inspired Real-World Anime Super-Resolution","Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao","Image and Video Processing (eess.IV)","While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we introduce the balanced twin perceptual loss combining both anime and photorealistic high-level features to mitigate unwanted color artifacts and increase visual clarity. We evaluate our method through extensive experiments on the public benchmark, showing our method outperforms state-of-the-art approaches by a large margin.","Sun, 3 Mar 2024 19:52:43 UTC (20,353 KB)"
"30","The Case for Animal-Friendly AI","Sankalpa Ghose, Yip Fai Tse, Kasra Rasaee, Jeff Sebo, Peter Singer","Artificial Intelligence (cs.AI)","Artificial intelligence is seen as increasingly important, and potentially profoundly so, but the fields of AI ethics and AI engineering have not fully recognized that these technologies, including large language models (LLMs), will have massive impacts on animals. We argue that this impact matters, because animals matter morally.
As a first experiment in evaluating animal consideration in LLMs, we constructed a proof-of-concept Evaluation System, which assesses LLM responses and biases from multiple perspectives. This system evaluates LLM outputs by two criteria: their truthfulness, and the degree of consideration they give to the interests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using a set of structured queries and predefined normative perspectives. Preliminary results suggest that the outcomes of the tested models can be benchmarked regarding the consideration they give to animals, and that generated positions and biases might be addressed and mitigated with more developed and validated systems.
Our research contributes one possible approach to integrating animal ethics in AI, opening pathways for future studies and practical applications in various fields, including education, public policy, and regulation, that involve or relate to animals and society. Overall, this study serves as a step towards more useful and responsible AI systems that better recognize and respect the vital interests and perspectives of all sentient beings.","Sat, 2 Mar 2024 12:41:11 UTC (1,708 KB)"
"31","Shaping Multi-Robot Patrol Performance with Heterogeneity in Individual Learning Behavior","Connor York, Zachary R Madin, Paul O'Dowd, Edmund R Hunt","Robotics (cs.RO)","Individual differences in learning behavior within social groups, whether in humans, other animals, or among robots, can have significant effects on collective task performance. This is because it can affect individuals' response to the environment and their interactions with each other. In recent years there has been rising interest in the question of how individual differences, whether in learning or other traits, affect collective outcomes: studied, for example, in social insect foraging behavior. Multi-robot, 'swarm' systems have a heritage of bioinspiration from such examples, and here we consider whether heterogeneity in a learning behavior called latent inhibition (LI) may be useful for a team of patrolling robots tasked with environmental monitoring and anomaly detection. Individuals with high LI can be seen as better at learning to be inattentive to irrelevant or unrewarding stimuli, while low LI individuals might be seen as 'distractible' and yet, more positively, more exploratory. We introduce a simple model of the effects of LI as the probability of re-searching a location for a reward (anomalous reading) where it has previously been found to be unrewarding (irrelevant). In simulated patrols, we find that a negatively skewed distribution of mostly high LI robots, and just a single low LI robot, is collectively most effective at monitoring dynamic environments. These results are an example of 'functional heterogeneity' in 'swarm engineering' and could inform predictions for ecological distributions of learning traits within social groups.","Sat, 2 Mar 2024 11:29:09 UTC (983 KB)"
"32","Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine","Yuhong Huang, Zhenshan Bing, Zitao Zhang, Genghang Zhuang, Kai Huang, Alois Knoll","Robotics (cs.RO)","Balancing oneself using the spine is a physiological alignment of the body posture in the most efficient manner by the muscular forces for mammals. For this reason, we can see many disabled quadruped animals can still stand or walk even with three limbs. This paper investigates the optimization of dynamic balance during trot gait based on the spatial relationship between the center of mass (CoM) and support area influenced by spinal flexion. During trotting, the robot balance is significantly influenced by the distance of the CoM to the support area formed by diagonal footholds. In this context, lateral spinal flexion, which is able to modify the position of footholds, holds promise for optimizing balance during trotting. This paper explores this phenomenon using a rat robot equipped with a soft actuated spine. Based on the lateral flexion of the spine, we establish a kinematic model to quantify the impact of spinal flexion on robot balance during trot gait. Subsequently, we develop an optimized controller for spinal flexion, designed to enhance balance without altering the leg locomotion. The effectiveness of our proposed controller is evaluated through extensive simulations and physical experiments conducted on a rat robot. Compared to both a non-spine based trot gait controller and a trot gait controller with lateral spinal flexion, our proposed optimized controller effectively improves the dynamic balance of the robot and retains the desired locomotion during trotting.","Fri, 1 Mar 2024 19:44:25 UTC (33,230 KB)"
"33","SURE: SUrvey REcipes for building reliable and robust deep networks","Yuting Li, Yingyi Chen, Xuanlong Yu, Dexiong Chen, Xi Shen","Computer Vision and Pattern Recognition (cs.CV)","In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques--spanning model regularization, classifier and optimization--substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the benchmark of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new benchmark for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \url{this https URL}.","Fri, 1 Mar 2024 13:58:19 UTC (2,820 KB)"
"34","Spatio-temporal reconstruction of substance dynamics using compressed sensing in multi-spectral magnetic resonance spectroscopic imaging","Utako Yamamoto, Hirohiko Imai, Kei Sano, Masayuki Ohzeki, Tetsuya Matsuda, Toshiyuki Tanaka","Signal Processing (eess.SP)","The objective of our study is to observe dynamics of multiple substances in vivo with high temporal resolution from multi-spectral magnetic resonance spectroscopic imaging (MRSI) data. The multi-spectral MRSI can effectively separate spectral peaks of multiple substances and is useful to measure spatial distributions of substances. However it is difficult to measure time-varying substance distributions directly by ordinary full sampling because the measurement requires a significantly long time. In this study, we propose a novel method to reconstruct the spatio-temporal distributions of substances from randomly undersampled multi-spectral MRSI data on the basis of compressed sensing (CS) and the partially separable function model with base spectra of substances. In our method, we have employed spatio-temporal sparsity and temporal smoothness of the substance distributions as prior knowledge to perform CS. The effectiveness of our method has been evaluated using phantom data sets of glass tubes filled with glucose or lactate solution in increasing amounts over time and animal data sets of a tumor-bearing mouse to observe the metabolic dynamics involved in the Warburg effect in vivo. The reconstructed results are consistent with the expected behaviors, showing that our method can reconstruct the spatio-temporal distribution of substances with a temporal resolution of four seconds which is extremely short time scale compared with that of full sampling. Since this method utilizes only prior knowledge naturally assumed for the spatio-temporal distributions of substances and is independent of the number of the spectral and spatial dimensions or the acquisition sequence of MRSI, it is expected to contribute to revealing the underlying substance dynamics in MRSI data already acquired or to be acquired in the future.","Fri, 1 Mar 2024 09:46:41 UTC (2,984 KB)"
"35","From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic Perching","Bryan Habas, Bo Cheng","Robotics (cs.RO)","Inverted landing is a routine behavior among a number of animal fliers. However, mastering this feat poses a considerable challenge for robotic fliers, especially to perform dynamic perching with rapid body rotations (or flips) and landing against gravity. Inverted landing in flies have suggested that optical flow senses are closely linked to the precise triggering and control of body flips that lead to a variety of successful landing behaviors. Building upon this knowledge, we aimed to replicate the flies' landing behaviors in small quadcopters by developing a control policy general to arbitrary ceiling-approach conditions. First, we employed reinforcement learning in simulation to optimize discrete sensory-motor pairs across a broad spectrum of ceiling-approach velocities and directions. Next, we converted the sensory-motor pairs to a two-stage control policy in a continuous augmented-optical flow space. The control policy consists of a first-stage Flip-Trigger Policy, which employs a one-class support vector machine, and a second-stage Flip-Action Policy, implemented as a feed-forward neural network. To transfer the inverted-landing policy to physical systems, we utilized domain randomization and system identification techniques for a zero-shot sim-to-real transfer. As a result, we successfully achieved a range of robust inverted-landing behaviors in small quadcopters, emulating those observed in flies.","Thu, 29 Feb 2024 21:09:08 UTC (19,562 KB)"
"36","Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization","Ganga Gireesan, Nisha Pillai, Michael J Rothrock, Bindu Nanduri, Zhiqian Chen, Mahalingam Ramkumar","Machine Learning (cs.LG)","Pathogen control is a critical aspect of modern poultry farming, providing important benefits for both public health and productivity. Effective poultry management measures to reduce pathogen levels in poultry flocks promote food safety by lowering risks of food-borne illnesses. They also support animal health and welfare by preventing infectious diseases that can rapidly spread and impact flock growth, egg production, and overall health. This study frames the search for optimal management practices that minimize the presence of multiple pathogens as a combinatorial optimization problem. Specifically, we model the various possible combinations of management settings as a solution space that can be efficiently explored to identify configurations that optimally reduce pathogen levels. This design incorporates a neural network feedback-based method that combines feature explanations with global sensitivity analysis to ensure combinatorial optimization in multiobjective settings. Our preliminary experiments have promising results when applied to two real-world agricultural datasets. While further validation is still needed, these early experimental findings demonstrate the potential of the model to derive targeted feature interactions that adaptively optimize pathogen control under varying real-world constraints.","Wed, 28 Feb 2024 02:15:47 UTC (3,504 KB)"
"37","Learning a Generalized Physical Face Model From Data","Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley","Computer Vision and Pattern Recognition (cs.CV)","Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a simulation-free manner. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.","Thu, 29 Feb 2024 18:59:31 UTC (8,551 KB)"
"38","3D Gaussian Model for Animation and Texturing","Xiangzhi Eric Wang, Zackary P. T. Sin","Graphics (cs.GR)","3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance. Despite this achievement, however, it is not readily applicable to developing interactive applications. Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model. We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer. By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications. It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering. Our work, on the other hand, only requires an approximated geometry for rendering an object in high fidelity. Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians. We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting.","Thu, 29 Feb 2024 18:43:43 UTC (24,864 KB)"
"39","Properties of Hagen-Poiseuille flows in channel networks","A. F. Valente, R. Almeida, R. Dilão","Cell Behavior (q-bio.CB)","We derive the main properties of adaptive Hagen-Poiseuille flows in elastic microchannel networks akin to biological veins in organisms. We show that adaptive Hagen-Poiseuille flows successfully simulate key features of \textit{Physarum polycephalum} networks, replicating physiological out-of-equilibrium phenomena like peristalsis and shuttle streaming, associated with the mechanism of nutrient transport in \textit{Physarum}. A new topological steady state has been identified for asynchronous adaptation, supporting out-of-equilibrium laminar fluxes. Adaptive Hagen-Poiseuille flows show saturation effects on the fluxes in contractile veins, as observed in animal and artificial contractile veins.","Thu, 29 Feb 2024 14:09:40 UTC (4,320 KB)"
"40","Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning","Jin Hwa Lee, Stefano Sarao Mannelli, Andrew Saxe","Neurons and Cognition (q-bio.NC)","Diverse studies in systems neuroscience begin with extended periods of training known as 'shaping' procedures. These involve progressively studying component parts of more complex tasks, and can make the difference between learning a task quickly, slowly or not at all. Despite the importance of shaping to the acquisition of complex tasks, there is as yet no theory that can help guide the design of shaping procedures, or more fundamentally, provide insight into its key role in learning. Modern deep reinforcement learning systems might implicitly learn compositional primitives within their multilayer policy networks. Inspired by these models, we propose and analyse a model of deep policy gradient learning of simple compositional reinforcement learning tasks. Using the tools of statistical physics, we solve for exact learning dynamics and characterise different learning strategies including primitives pre-training, in which task primitives are studied individually before learning compositional tasks. We find a complex interplay between task complexity and the efficacy of shaping strategies. Overall, our theory provides an analytical understanding of the benefits of shaping in a class of compositional tasks and a quantitative account of how training protocols can disclose useful task primitives, ultimately yielding faster and more robust learning.","Wed, 28 Feb 2024 14:33:09 UTC (2,889 KB)"
"41","FSL Model can Score Higher as It Is","Yunwei Bai, Ying Kiat Tan, Tsuhan Chen","Computer Vision and Pattern Recognition (cs.CV)","In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.","Wed, 28 Feb 2024 12:37:30 UTC (1,944 KB)"
"42","Pach's animal problem within the bounding box","Martin Tancer","Combinatorics (math.CO)","A collection of unit cubes with integer coordinates in $\mathbb R^3$ is an animal if its union is homeomorphic to the 3-ball. Pach's animal problem asks whether any animal can be transformed to a single cube by adding or removing cubes one by one in such a way that any intermediate step is an animal as well. Here we provide an example of an animal that cannot be transformed to a single cube this way within its bounding box.","Wed, 28 Feb 2024 10:04:16 UTC (149 KB)"
"43","From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs","Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, Hang Xu","Artificial Intelligence (cs.AI)","The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.","Wed, 28 Feb 2024 08:42:23 UTC (1,035 KB)"
"44","Prosocial and Financial Incentives for Biodiversity Conservation: A Field Experiment Using a Smartphone App","Shusaku Sasaki, Takahiro Kubo, Shodai Kitano","General Economics (econ.GN)","Ascertaining the number, type, and location of plant, insect, and animal species is essential for biodiversity conservation. However, comprehensively monitoring the situation only through public fixed-point surveys is challenging, and therefore information voluntarily provided by citizens assists in ascertaining the species distribution. To effectively encourage the citizens' data sharing behavior, this study proposed a prosocial incentive scheme in which, if they provide species information, donations are made to activities for saving endangered species. We conducted a field experiment with users (N=830) of a widely-used Japanese smartphone app to which they post species photos and measured the incentive's effect on their posting behavior. In addition, we measured the effect of a financial incentive scheme that provides monetary rewards for posting species photos and compared the two incentives' effects. The analyses revealed that while the prosocial incentive did not increase the number of posts on average, it did change the contents of posts, increasing the proportion of posts on rare species. On the contrary, the financial incentive statistically significantly increased the number of posts, in particular, on less rare and invasive species. Our findings suggest that the prosocial and financial incentives could stimulate different motivations and encourage different posting behaviors.","Wed, 28 Feb 2024 04:51:21 UTC (659 KB)"
"45","EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions","Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo","Computer Vision and Pattern Recognition (cs.CV)","In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.","Tue, 27 Feb 2024 13:10:11 UTC (18,549 KB)"
"46","CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization","Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu","Computer Vision and Pattern Recognition (cs.CV)","In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.","Tue, 27 Feb 2024 05:10:59 UTC (29,118 KB)[v2] Wed, 28 Feb 2024 08:13:38 UTC (29,118 KB)"
"47","Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure","Albert J. Wakhloo, Will Slatton, SueYeon Chung","Neurons and Cognition (q-bio.NC)","Humans and animals can recognize latent structures in their environment and apply this information to efficiently navigate the world. Several recent works argue that the brain supports these abilities by forming neural representations that encode such latent structures in flexible, generalizable ways. However, it remains unclear what aspects of neural population activity are contributing to these computational capabilities. Here, we develop an analytical theory linking the mesoscopic statistics of a neural population's activity to generalization performance on a multi-task learning problem. To do this, we rely on a generative model in which different tasks depend on a common, unobserved latent structure and predictions are formed from a linear readout of a neural population's activity. We show that three geometric measures of the population activity determine generalization performance in these settings. Using this theory, we find that experimentally observed factorized (or disentangled) representations naturally emerge as an optimal solution to the multi-task learning problem. We go on to show that when data is scarce, optimal codes compress less informative latent variables, and when data is abundant, optimal codes expand this information in the state space. We validate predictions from our theory using biological and artificial neural network data. Our results therefore tie neural population geometry to the multi-task learning problem and make normative predictions of the structure of population activity in these settings.","Mon, 26 Feb 2024 17:39:23 UTC (4,469 KB)"
"48","Recent progress in the physical principles of dynamic ground self-righting","Chen Li","Biological Physics (physics.bio-ph)","Animals and robots must be able to self-right on the ground after flipping over to survive or operate. Biology research has described various strategies and measured motor control patterns in many species. Robotics research has designed many mechanisms to enable ground self-righting. However, the physical principles governing ground self-righting transitions are relatively less known, except limited understanding in turtles with rigid shell in two dimensions. Here I review recent progress which I have led in advancing this understanding using cockroaches as model organisms, by integrating biology experiments, robotic modeling, and physics modeling.","Mon, 26 Feb 2024 17:08:14 UTC (1,825 KB)"
"49","Program-Based Strategy Induction for Reinforcement Learning","Carlos G. Correa, Thomas L. Griffiths, Nathaniel D. Daw","Machine Learning (cs.LG)","Typical models of learning assume incremental estimation of continuously-varying decision variables like expected rewards. However, this class of models fails to capture more idiosyncratic, discrete heuristics and strategies that people and animals appear to exhibit. Despite recent advances in strategy discovery using tools like recurrent networks that generalize the classic models, the resulting strategies are often onerous to interpret, making connections to cognition difficult to establish. We use Bayesian program induction to discover strategies implemented by programs, letting the simplicity of strategies trade off against their effectiveness. Focusing on bandit tasks, we find strategies that are difficult or unexpected with classical incremental learning, like asymmetric learning from rewarded and unrewarded trials, adaptive horizon-dependent random exploration, and discrete state switching.","Mon, 26 Feb 2024 15:40:46 UTC (141 KB)"
"50","Domain Embeddings for Generating Complex Descriptions of Concepts in Italian Language","Alessandro Maisto","Computation and Language (cs.CL)","In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory. Recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning. This often involves decoding the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques. Our approach introduces an alternative strategy grounded in linguistic data. We have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of Italian nouns categorized into 4 semantic traits and 20 concrete noun sub-categories, and a list of Italian verbs classified according to their semantic classes. In these matrices, the co-occurrence values for each word are calculated exclusively with a defined set of words pertinent to a particular lexical domain. The resource comprises 21 domain-specific matrices, one comprehensive matrix, and a Graphical User Interface. Our model facilitates the generation of reasoned semantic descriptions of concepts by selecting matrices directly associated with concrete conceptual knowledge, such as a matrix based on location nouns and the concept of animal habitats. We assessed the utility of the resource through two experiments, achieving promising outcomes in both: the automatic classification of animal nouns and the extraction of animal features.","Mon, 26 Feb 2024 15:04:35 UTC (2,693 KB)"
"51","Cinematographic Camera Diffusion Model","Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, Baoquan Chen","Graphics (cs.GR)","Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators. Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters. Dealing with these possibilities is part of the complexity of the problem. While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control.
In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers. We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists. The code and data are available at \URL{this https URL}.","Sun, 25 Feb 2024 16:48:25 UTC (48,020 KB)"
"52","Energy-conserving intermittent-contact motion in complex models","Sergey Pankov","Robotics (cs.RO)","Some mechanical systems, that are modeled to have inelastic collisions, nonetheless possess energy-conserving intermittent-contact solutions, known as collisionless solutions. Such a solution, representing a persistent hopping or walking across a level ground, may be important for understanding animal locomotion or for designing efficient walking machines. So far, collisionless motion has been analytically studied in simple two degrees of freedom (DOF) systems, or in a system that decouples into 2-DOF subsystems in the harmonic approximation. In this paper we extend the consideration to a N-DOF system, recovering the known solutions as a special N = 2 case of the general formulation. We show that in the harmonic approximation the collisionless solution is determined by the spectrum of the system. We formulate a solution existence condition, which requires the presence of at least one oscillating normal mode in the most constrained phase of the motion. An application of the developed general framework is illustrated by finding a collisionless solution for a rocking motion of a biped with an armed standing torso.","Sun, 25 Feb 2024 06:59:40 UTC (212 KB)"
"53","CharacterMixer: Rig-Aware Interpolation of 3D Characters","Xiao Zhan, Rao Fu, Daniel Ritchie","Graphics (cs.GR)","We present CharacterMixer, a system for blending two rigged 3D characters with different mesh and skeleton topologies while maintaining a rig throughout interpolation. CharacterMixer also enables interpolation during motion for such characters, a novel feature. Interpolation is an important shape editing operation, but prior methods have limitations when applied to rigged characters: they either ignore the rig (making interpolated characters no longer posable) or use a fixed rig and mesh topology. To handle different mesh topologies, CharacterMixer uses a signed distance field (SDF) representation of character shapes, with one SDF per bone. To handle different skeleton topologies, it computes a hierarchical correspondence between source and target character skeletons and interpolates the SDFs of corresponding bones. This correspondence also allows the creation of a single ""unified skeleton"" for posing and animating interpolated characters. We show that CharacterMixer produces qualitatively better interpolation results than two state-of-the-art methods while preserving a rig throughout interpolation.","Fri, 23 Feb 2024 19:39:24 UTC (44,841 KB)"
"54","Morphological Symmetries in Robotics","Daniel Ordoñez-Apraez, Giulio Turrisi, Vladimir Kostic, Mario Martin, Antonio Agudo, Francesc Moreno-Noguer, Massimiliano Pontil, Claudio Semini, Carlos Mastalli","Robotics (cs.RO)","We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models through data augmentation, or by applying equivariant/invariant constraints on the model's architecture. In the context of analytical methods, we employ abstract harmonic analysis to decompose the robot's dynamics into a superposition of lower-dimensional, independent dynamics. We substantiate our claims with both synthetic and real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we introduce the repository MorphoSymm to facilitate the practical use of the theory and applications outlined in this work.","Fri, 23 Feb 2024 17:21:21 UTC (33,883 KB)"
"55","Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks","Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet","Computer Vision and Pattern Recognition (cs.CV)","Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function. This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep convolutional neural networks (CNNs). Retinotopic mapping was integrated into the inputs of standard off-the-shelf convolutional neural networks (CNNs), which were then retrained on the ImageNet task. As expected, the logarithmic-polar mapping improved the network's ability to handle arbitrary image zooms and rotations, particularly for isolated objects. Surprisingly, the retinotopically mapped network achieved comparable performance in classification. Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted. This replicates a crucial ability of the human visual system that is absent in typical convolutional neural networks (CNNs). These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes.","Fri, 23 Feb 2024 18:15:37 UTC (33,786 KB)"
"56","Charting Ethical Tensions in Multispecies Technology Research through Beneficiary-Epistemology Space","Steve Benford, Clara Mancini, Alan Chamberlain, Eike Schneiders, Simon Castle-Green, Joel Fischer, Ayse Kucukyilmaz, Guido Salimbeni, Victor Ngo, Pepita Barnard, Matt Adams, Nick Tandavanitj, Ju Row Farr","Human-Computer Interaction (cs.HC)","While ethical challenges are widely discussed in HCI, far less is reported about the ethical processes that researchers routinely navigate. We reflect on a multispecies project that negotiated an especially complex ethical approval process. Cat Royale was an artist-led exploration of creating an artwork to engage audiences in exploring trust in autonomous systems. The artwork took the form of a robot that played with three cats. Gaining ethical approval required an extensive dialogue with three Institutional Review Boards (IRBs) covering computer science, veterinary science and animal welfare, raising tensions around the welfare of the cats, perceived benefits and appropriate methods, and reputational risk to the University. To reveal these tensions we introduce beneficiary-epistemology space, that makes explicit who benefits from research (humans or animals) and underlying epistemologies. Positioning projects and IRBs in this space can help clarify tensions and highlight opportunities to recruit additional expertise.","Fri, 23 Feb 2024 16:57:39 UTC (1,973 KB)"
"57","Designing Multispecies Worlds for Robots, Cats, and Humans","Eike Schneiders, Steve Benford, Alan Chamberlain, Clara Mancini, Simon Castle-Green, Victor Ngo, Ju Row Farr, Matt Adams, Nick Tandavanitj, Joel Fischer","Human-Computer Interaction (cs.HC)","We reflect on the design of a multispecies world centred around a bespoke enclosure in which three cats and a robot arm coexist for six hours a day during a twelve-day installation as part of an artist-led project. In this paper, we present the project's design process, encompassing various interconnected components, including the cats, the robot and its autonomous systems, the custom end-effectors and robot attachments, the diverse roles of the humans-in-the-loop, and the custom-designed enclosure. Subsequently, we provide a detailed account of key moments during the deployment and discuss the design implications for future multispecies systems. Specifically, we argue that designing the technology and its interactions is not sufficient, but that it is equally important to consider the design of the `world' in which the technology operates. Finally, we highlight the necessity of human involvement in areas such as breakdown recovery, animal welfare, and their role as audience.","Fri, 23 Feb 2024 16:51:09 UTC (15,556 KB)"
"58","Homeostatic motion planning with innate physics knowledge","Giulia Lafratta, Bernd Porr, Christopher Chandler, Alice Miller","Robotics (cs.RO)","Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called ""tasks"", each representing a closed-loop behaviour. We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept.","Fri, 23 Feb 2024 15:30:57 UTC (5,313 KB)"
"59","All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data","Amanda K. Navine, Tom Denton, Matthew J. Weldy, Patrick J. Hart","Quantitative Methods (q-bio.QM)","Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more. Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool. In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count. The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset. In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score. Our approach targets a desirable ecological estimator and provides a more rigorous grounding for identifying the core problems caused by distribution shifts -- when the defining characteristics of the data distribution change -- and designing strategies to mitigate them. We propose a validation scheme for estimating call density in a body of data and obtain, through Bayesian reasoning, probability distributions of confidence scores for both the positive and negative classes. We use these distributions to predict site-level densities, which may be subject to distribution shifts. We test our proposed methods on a real-world study of Hawaiian birds and provide simulation results leveraging existing fully annotated datasets, demonstrating robustness to variations in call density and classifier model quality.","Fri, 23 Feb 2024 14:52:44 UTC (297 KB)"
"60","Elastic interactions compete with persistent cell motility to drive durotaxis","Subhaya Bose, Haiqin Wang, Xinpeng Xu, Arvind Gopinath, Kinjal Dasbiswas","Soft Condensed Matter (cond-mat.soft)","Many animal cells crawling on elastic substrates exhibit durotaxis and have implications in several biological processes including tissue development, and tumor progression. Here, we introduce a phenomenological model for durotactic migration incorporating both elastic deformation-mediated cell-substrate interactions and the stochasticity of cell migration. Our model is motivated by the key observation in one of the first demonstrations of durotaxis: a single contractile cell at an interface between a softer and a stiffer region of an elastic substrate reorients and migrates towards the stiffer region. We model migrating cells as self-propelling, persistently motile agents that exert contractile, dipolar traction forces on the underlying elastic substrate. The resulting substrate deformations induce elastic interactions with mechanical boundaries, captured by an elastic potential that depends on cell position and orientation relative to the boundary. The potential is attractive or repulsive depending on whether the mechanical boundary condition is clamped or free, which represents the cell being on the softer or stiffer side, respectively, of a confining boundary. The forces and torques from the interactions drive cells to orient perpendicular (parallel) to the boundary and accumulate (deplete) at the clamped (free) boundary, extent of which is determined by elastic potential (A) and motility (Pe). While the elastic interaction drives durotaxis, cell migratory movements such as random reorientation and self-propulsion enable the cell from the attractive potential thereby reducing durotaxis. We define metrics quantifying boundary accumulation and durotaxis and present a phase diagram that identifies three possible regimes: durotaxis, adurotaxis without accumulation and adurotaxis with motility-induced accumulation at a confining boundary.","Fri, 23 Feb 2024 01:02:24 UTC (4,186 KB)"
"61","Object permanence in newborn chicks is robust against opposing evidence","Justin N. Wood, Tomer D. Ullman, Brian W. Wood, Elizabeth S. Spelke, Samantha M. W. Wood","Neurons and Cognition (q-bio.NC)","Newborn animals have advanced perceptual skills at birth, but the nature of this initial knowledge is unknown. Is initial knowledge flexible, continuously adapting to the statistics of experience? Or can initial knowledge be rigid and robust to change, even in the face of opposing evidence? We address this question through controlled-rearing experiments on newborn chicks. First, we reared chicks in an impoverished virtual world, where objects never occluded one another, and found that chicks still succeed on object permanence tasks. Second, we reared chicks in a virtual world in which objects teleported from one location to another while out of view: an unnatural event that violates the continuity of object motion. Despite seeing thousands of these violations of object permanence, and not a single non-violation, the chicks behaved as if object permanence were true, exhibiting the same behavior as chicks reared with natural object permanence events. We conclude that object permanence develops prenatally and is robust to change from opposing evidence.","Thu, 22 Feb 2024 15:39:29 UTC (545 KB)"
"62","Semantic Image Synthesis with Unconditional Generator","Jungwoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, Youngjung Uh","Computer Vision and Pattern Recognition (cs.CV)","Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks. Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models. Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks. The proxy masks are prepared from the feature maps of random samples in the generator by simple clustering. The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples. Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks. Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo. Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings.","Thu, 22 Feb 2024 09:10:28 UTC (45,393 KB)"
"63","The Architecture of Sponge Choanocyte Chambers Maximizes Mechanical Pumping Efficiency","Takumi Ogawa, Shuji Koyama, Toshihiro Omori, Kenji Kikuchi, Helene de Maleprade, Raymond E. Goldstein, Takuji Ishikawa","Soft Condensed Matter (cond-mat.soft)","Sponges, the basalmost members of the animal kingdom, exhibit a range of complex architectures in which microfluidic channels connect multitudes of spherical chambers lined with choanocytes, flagellated filter-feeding cells. Choanocyte chambers can possess scores or even hundreds of such cells, which drive complex flows entering through porous walls and exiting into the sponge channels. One of the mysteries of the choanocyte chamber is its spherical shape, as it seems inappropriate for inducing directional transport since many choanocyte flagella beat in opposition to such a flow. Here we combine direct imaging of choanocyte chambers in living sponges with computational studies of many-flagella models to understand the connection between chamber architecture and directional flow. We find that those flagella that beat against the flow play a key role in raising the pressure inside the choanocyte chamber, with the result that the mechanical pumping efficiency, calculated from the pressure rise and flow rate, reaches a maximum at a small outlet opening angle. Comparison between experimental observations and the results of numerical simulations reveal that the chamber diameter, flagellar wave number and the outlet opening angle of the freshwater sponge $E. muelleri$, as well as several other species, are related in a manner that maximizes the mechanical pumping efficiency. These results indicate the subtle balances at play during morphogenesis of choanocyte chambers, and give insights into the physiology and body design of sponges.","Thu, 22 Feb 2024 08:05:05 UTC (5,462 KB)"
"64","Strategies and safety simulations for ultrasonic cervical spinal cord neuromodulation","Rui Xu, Sven Bestmann, Bradley E. Treeby, Eleanor Martin","Medical Physics (physics.med-ph)","Focused ultrasound spinal cord neuromodulation studies have demonstrated spinal cord neuromodulation in small animals. The safe and efficacious translation of these approaches to human scale requires an understanding of ultrasound propagation and heat deposition within the human spine. To address this, combined acoustic and thermal modelling was used to assess the pressure and heat distributions produced by a 500 kHz source focused to the C5/C6 level of the cervical spine via two approaches a) the posterior acoustic window between vertebral posterior arches, or b) the lateral intervertebral foramen from which the C6 spinal nerve exits. Pulse trains of 150 0.1 s pulses with a pulse repetition frequency of 0.33 Hz and free-field spatial peak pulse-averaged intensity of 10 W/cm^2 were simulated for the CT volumes of four subjects and for $\pm$10 mm translational and $\pm$10° rotational source positioning errors. Target pressures ranged between 20% and 70% of free-field spatial peak pressures with the posterior approach, and 20% and 100% with the lateral approach. When the source was optimally positioned with the posterior approach, peak spine heating values were below 1°C, but source mis-positioning resulted in bone heating up to 4°C. Heating with the lateral approach did not exceed 2°C within the mispositioning range. There were substantial inter-subject differences in target pressures and peak heating values. Target pressure varied three to four-fold between subjects, depending on approach, while peak heating varied approximately two-fold between subjects. This results in a near ten-fold range in the target pressure achieved per degree of peak heating between subjects. This highlights the importance of developing trans-spine ultrasound simulation software for the assurance of subject-specific safety and efficacy of focused ultrasound spinal cord therapies.","Wed, 21 Feb 2024 20:34:52 UTC (5,768 KB)"
"65","Learning dynamic representations of the functional connectome in neurobiological networks","Luciano Dyballa, Samuel Lang, Alexandra Haslund-Gourley, Eviatar Yemini, Steven W. Zucker","Neurons and Cognition (q-bio.NC)","The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time codes the different experimental variables (e.g., application of chemical stimuli), this provides an atlas of neural motifs active during separate stages of an experiment (e.g., stimulus application or spontaneous behaviors). Results from our analysis are experimentally validated, confirming that our method is able to robustly predict causal interactions between neurons to generate behavior. Code is available at this https URL.","Wed, 21 Feb 2024 19:54:25 UTC (10,680 KB)[v2] Tue, 27 Feb 2024 19:54:21 UTC (10,680 KB)"
"66","Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters","Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian","Human-Computer Interaction (cs.HC)","Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.","Wed, 21 Feb 2024 11:35:20 UTC (2,332 KB)"
"67","Learning Highly Dynamic Behaviors for Quadrupedal Robots","Chong Zhang, Jiapeng Sheng, Tingguang Li, He Zhang, Cheng Zhou, Qingxu Zhu, Rui Zhao, Yizheng Zhang, Lei Han","Robotics (cs.RO)","Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with markers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet.","Wed, 21 Feb 2024 02:06:45 UTC (3,552 KB)"
"68","Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space","Hao-Wei Chung, Ching-Hao Chiu, Yu-Jen Chen, Yiyu Shi, Tsung-Yi Ho","Computer Vision and Pattern Recognition (cs.CV)","Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively.","Tue, 20 Feb 2024 14:56:28 UTC (1,507 KB)"
"69","BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization","Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu","Computer Vision and Pattern Recognition (cs.CV)","Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway this http URL achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.","Tue, 20 Feb 2024 07:11:27 UTC (5,377 KB)"
"70","Composite likelihood inference for space-time point processes","Abdollah Jalilian, Francisco Cuevas-Pacheco, Ganggang Xu, Rasmus Waagepetersen","Methodology (stat.ME)","The dynamics of a rain forest is extremely complex involving births, deaths and growth of trees with complex interactions between trees, animals, climate, and environment. We consider the patterns of recruits (new trees) and dead trees between rain forest censuses. For a current census we specify regression models for the conditional intensity of recruits and the conditional probabilities of death given the current trees and spatial covariates. We estimate regression parameters using conditional composite likelihood functions that only involve the conditional first order properties of the data. When constructing assumption lean estimators of covariance matrices of parameter estimates we only need mild assumptions of decaying conditional correlations in space while assumptions regarding correlations over time are avoided by exploiting conditional centering of composite likelihood score functions. Time series of point patterns from rain forest censuses are quite short while each point pattern covers a fairly big spatial region. To obtain asymptotic results we therefore use a central limit theorem for the fixed timespan - increasing spatial domain asymptotic setting. This also allows us to handle the challenge of using stochastic covariates constructed from past point patterns. Conveniently, it suffices to impose weak dependence assumptions on the innovations of the space-time process. We investigate the proposed methodology by simulation studies and applications to rain forest data.","Mon, 19 Feb 2024 21:19:46 UTC (3,775 KB)"
"71","Opening the black box of language acquisition","Jérôme Michaud, Anna Jon-and","Computation and Language (cs.CL)","Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process. Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages.","Sun, 18 Feb 2024 19:11:58 UTC (997 KB)"
"72","Dynamic planning in hierarchical active inference","Matteo Priorelli, Ivilin Peev Stoianov","Artificial Intelligence (cs.AI)","By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents. We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section. This study distances itself from traditional views centered on neural networks and reinforcement learning, and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models.","Sun, 18 Feb 2024 17:32:53 UTC (4,558 KB)"
"73","Superradiant pulse saturation in a Free Electron Laser","Pornthep Pongchalee, Brian W.J. McNeil","Accelerator Physics (physics.acc-ph)","A study is made of the saturation mechanism of a single superradiant spike of radiation in a Free Electron Laser. A one-dimensional (1D) computer model is developed using the Puffin, un-averaged FEL simulation code, which allows sub-radiation wavelength evolution of both the spike radiation field and the electron dynamics to be modelled until the highly non-linear saturation process of the spike is observed. Animations of the process from the start to the end of the interaction are available. The resultant saturated spike duration is at the sub-wavelength scale and has a broad spectrum. The electrons passing through the spike can both lose and gain energy many times greater than that of the normal non-pulsed FEL interaction. A saturation mechanism is proposed and tested via a simple analysis of the 1D FEL equations. The scaling results of the analysis are seen to be in good agreement with the numerical results. A simple model of three dimensional diffraction effects of the radiation is applied to the results of the 1D simulations. This greatly reduces longer wavelengths of the power spectrum, which are seen to be emitted mainly after the electrons have propagated through the spike, and is seen to be in qualitative agreement with recent experimental results.","Fri, 16 Feb 2024 19:15:58 UTC (15,305 KB)"
"74","GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians","Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu","Graphics (cs.GR)","Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the ""GaussianHair Scattering Model"", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.","Fri, 16 Feb 2024 07:13:24 UTC (32,069 KB)"
"75","UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style","Yan Kang, Hao Lin, Mingjian Yang, Shin-Jye Lee","Information Retrieval (cs.IR)","The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledge that composes a variety of domain text pairs from multilingual mappings, entity relationships, and term explanation perspectives, respectively. In the multi-modal fusion phase, we novelly propose a user-aware multi-modal contribution measurement mechanism to weight multi-modal features dynamically according to user features at the interaction level and employ the DCN-V2 module to model bounded-degree multi-modal crosses effectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large real-world datasets, demonstrating substantial performance enhancements.","Fri, 16 Feb 2024 00:25:53 UTC (16,127 KB)"
"76","SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms","Dian Chen, Paul Yang, Ing-Ray Chen, Dong Sam Ha, Jin-Hee Cho","Machine Learning (cs.LG)","We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we demonstrate that our FL-based monitoring system significantly outperforms existing methods in prediction accuracy, operational efficiency, system reliability (i.e., mean time between failures or MTBF), and social welfare maximization by the mechanism designer. Our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms. The experimental results show that SusFL significantly improves system performance, including a $10\%$ reduction in energy consumption, a $15\%$ increase in social welfare, and a $34\%$ rise in Mean Time Between Failures (MTBF), alongside a marginal increase in the global model's prediction accuracy.","Thu, 15 Feb 2024 19:15:15 UTC (2,626 KB)"
"77","Effect of the social environment on olfaction and social skills in WT and mouse model of autism","Caroline Gora (PRC), Ana Dudas (PRC), Lucas Court (PRC), Anil Annamneedi (PRC), Gaëlle Lefort (PRC), Thiago-Seike Picoreti-Nakahara (PRC), Nicolas Azzopardi (PRC), Adrien Acquistapace (PRC), Anne-Lyse Lainé (PRC, PRC), Anne-Charlotte Trouillet (PRC), Lucile Drobecq (PRC), Emmanuel Pecnard (PRC), Benoit Piegu (PRC, PRC), Pascale Crépieux (PRC, PRC), Pablo Chamero (PRC), Lucie P. Pellissier (PRC)","Neurons and Cognition (q-bio.NC)","Autism spectrum disorders are complex, polygenic and heterogenous neurodevelopmental conditions, imposing a substantial economic burden. Genetics are influenced by the environment, specifically the social experience during the critical neurodevelopmental period. Despite efficacy of early behavior interventions targeted specific behaviors in some autistic children, there is no sustainable treatment for the two core symptoms: deficits in social interaction and communication, and stereotyped or restrained behaviors or interests. In this study, we investigated the impact of the social environment on both wild-type (WT) and Shank3 knockout (KO) mice, a mouse model that reproduces core autism-like symptoms. Our findings revealed that WT mice raised in an enriched social environment maintained social interest towards new conspecifics across multiple trials. Additionally, we observed that 2 hours or chronic social isolation induced social deficits or enhanced social interaction and olfactory neuron responses in WT animals, respectively. Notably, chronic social isolation restored both social novelty and olfactory deficits, and normalized self-grooming behavior in Shank3 KO mice. These results novel insights for the implementation of behavioral intervention and inclusive classrooms programs for children with ASD.","Wed, 29 Nov 2023 16:46:15 UTC (34,454 KB)"
"78","Lester: rotoscope animation through video object segmentation and tracking","Ruben Tous","Computer Vision and Pattern Recognition (cs.CV)","This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.","Thu, 15 Feb 2024 11:15:54 UTC (10,402 KB)"
"79","Solid Waste Detection in Remote Sensing Images: A Survey","Piero Fraternali, Luca Morandini, Sergio Luis Herrera González","Computer Vision and Pattern Recognition (cs.CV)","The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills. This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data. Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented. Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods.","Wed, 14 Feb 2024 10:24:04 UTC (491 KB)"
"80","On the Impact of Biological Risk in Aquaculture Valuation and Decision Making","Christian-Oliver Ewald, Kevin Kamm","Applications (stat.AP)","This paper explores the impact of stochastic mortality and disease on animal-based commodities, with a specific emphasis on aquaculture, particularly in the context of salmon farming. The investigation delves into the stochastic nature of mortality and treatment plans based on historical data related to salmon lice. Given that salmon lice pose a significant challenge in salmon farming, with associated treatment costs estimated to be comparable to feeding expenses, their removal is imperative to ensure the survival of the salmon and comply with the Norwegian government's stipulation of maintaining 0.5 lice per fish.
We propose a new model that considers the relationship between hosts and parasites to estimate the number of treatments required and their overall cost. An important aspect of this model is its incorporation of stochastic effectiveness for each removal. After calibrating the model to the available data, the study examines the stochastic behavior's impact on the optimal harvesting decision in comparison to deterministic mortality models. The results indicate an approximate $1.5\,\%$ increase in the value of the salmon farm when employing the harvesting rule based on the stochastic host-parasite model as opposed to a deterministic model.","Wed, 24 Jan 2024 09:47:47 UTC (1,083 KB)"
"81","A Wave-Corrected Assessment of the Local Midplane","Ziyuan Yin, Austin Hinkel","Astrophysics of Galaxies (astro-ph.GA)","As the number of known Galactic structures mounts thanks to the Gaia Space Telescope, it is now pertinent to study methods for disentangling structures occupying the same regions of the Milky Way. Indeed, understanding the precise form of each individual structure and the interactions between structures may aid in understanding their origins and chronology. Moreover, accounting for known structures allows one to probe still finer Galactic structure. In order to demonstrate, we have developed an odd low-pass filter (OLPF) which removes smaller, odd-parity structures like the vertical waves, and use the filtered data to examine the location of the Galaxy's mid-plane. We find that the radial wave identified by Xu et al. (2015) continues inward to at least the Sun's location, with an amplitude that decreases towards the inner, denser parts of the disk, consistent with a simple, qualitative simulation. Additionally, we employ the OLPF results to determine the solar offset, $z_{\odot}$, with smaller structures filtered out. We find that $z_{\odot} = 34.2 \pm 0.3$ pc.","Tue, 13 Feb 2024 17:52:46 UTC (5,323 KB)"
"82","ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents","Daksitha Withanage Don, Philipp Müller, Fabrizio Nunnari, Elisabeth André, Patrick Gebhard","Human-Computer Interaction (cs.HC)","Flexible and natural nonverbal reactions to human behavior remain a challenge for socially interactive agents (SIAs) that are predominantly animated using hand-crafted rules. While recently proposed machine learning based approaches to conversational behavior generation are a promising way to address this challenge, they have not yet been employed in SIAs. The primary reason for this is the lack of a software toolkit integrating such approaches with SIA frameworks that conforms to the challenging real-time requirements of human-agent interaction scenarios. In our work, we for the first time present such a toolkit consisting of three main components: (1) real-time feature extraction capturing multi-modal social cues from the user; (2) behavior generation based on a recent state-of-the-art neural network approach; (3) visualization of the generated behavior supporting both FLAME-based and Apple ARKit-based interactive agents. We comprehensively evaluate the real-time performance of the whole framework and its components. In addition, we introduce pre-trained behavioral generation models derived from psychotherapy sessions for domain-specific listening behaviors. Our software toolkit, pivotal for deploying and assessing SIAs' listening behavior in real-time, is publicly available. Resources, including code, behavioural multi-modal features extracted from therapeutic interactions, are hosted at this https URL","Mon, 12 Feb 2024 21:47:24 UTC (21,396 KB)"
"83","On Computationally Efficient Multi-Class Calibration","Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum","Machine Learning (cs.LG)","Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.
Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.","Mon, 12 Feb 2024 17:25:23 UTC (57 KB)"
"84","EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches","Pengcheng An, Jiawen Zhu, Zibo Zhang, Yifei Yin, Qingyuan Ma, Che Yan, Linghao Du, Jian Zhao","Human-Computer Interaction (cs.HC)","Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored ""Emotional Teasers""-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.","Sun, 11 Feb 2024 12:03:01 UTC (7,150 KB)"
"85","Squidgets: Sketch-based Widget Design and Direct Manipulation of 3D Scene","Joonho Kim, Karan Singh","Graphics (cs.GR)","Squidgets or 'sketch-widgets' is a novel stroke-based UI framework for direct scene manipulation. Squidgets is motivated by the observation that sketch strokes comprising visual abstractions of scene elements implicitly provide natural handles for the direct manipulation of scene parameters. Configurations of such strokes can further be explicitly drawn by users to author custom widgets associated with scene attributes. Users manipulate a scene by simply drawing strokes: a squidget is selected by partially matching the drawn stroke against both implicit scene contours and explicitly authored curves, and used in-situ to interactively control scene parameters associated with the squidget. We present an implementation of squidgets within the 3D modeling animation system Maya, and report on an evaluation of squidget creation and manipulation, by both casual users and professional artists.","Fri, 9 Feb 2024 21:40:23 UTC (34,436 KB)"
"86","HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting","Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang","Computer Vision and Pattern Recognition (cs.CV)","Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising outcomes obtained through 2D diffusion priors in recent works, current methods face challenges in achieving high-quality and animated avatars effectively. In this paper, we present $\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animated avatars from text prompts. Our method drives 3D Gaussians semantically to create a flexible and achievable appearance through the intermediate FLAME representation. Specifically, we incorporate the FLAME into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2) FLAME-based score distillation sampling, utilizing FLAME-based fine-grained control signal to guide score distillation from the text prompt. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting visually appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. They can be smoothly controlled by real-world speech and video. We hope that HeadStudio can advance digital avatar creation and that the present method can widely be applied across various domains.","Fri, 9 Feb 2024 02:58:37 UTC (10,449 KB)"
"87","Animated Stickers: Bringing Stickers to Life with Video Diffusion","David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu","Computer Vision and Pattern Recognition (cs.CV)","We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second.","Thu, 8 Feb 2024 22:49:32 UTC (5,685 KB)"
"88","Keyframer: Empowering Animation Design using Large Language Models","Tiffany Tseng, Ruijia Cheng, Jeffrey Nichols","Human-Computer Interaction (cs.HC)","Large language models (LLMs) have the potential to impact a wide range of creative domains, but the application of LLMs to animation is underexplored and presents novel challenges such as how users might effectively describe motion in natural language. In this paper, we present Keyframer, a design tool for animating static images (SVGs) with natural language. Informed by interviews with professional animation designers and engineers, Keyframer supports exploration and refinement of animations through the combination of prompting and direct editing of generated output. The system also enables users to request design variants, supporting comparison and ideation. Through a user study with 13 participants, we contribute a characterization of user prompting strategies, including a taxonomy of semantic prompt types for describing motion and a 'decomposed' prompting style where users continually adapt their goals in response to generated output.We share how direct editing along with prompting enables iteration beyond one-shot prompting interfaces common in generative tools today. Through this work, we propose how LLMs might empower a range of audiences to engage with animation creation.","Thu, 8 Feb 2024 21:43:30 UTC (4,146 KB)"
"89","DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer","Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei","Computer Vision and Pattern Recognition (cs.CV)","Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel.","Thu, 8 Feb 2024 14:39:16 UTC (9,938 KB)"
"90","A Mechanical Origin of Cooperative Transport","Eden Arbel, Naomi Oppenheimer, Yoav Lahini, Matan Yah Ben Zion","Soft Condensed Matter (cond-mat.soft)","Cooperative transport is a striking phenomenon where multiple agents join forces to transit a payload too heavy for the individual. While social animals such as ants are routinely observed to coordinate transport at scale, reproducing the effect in artificial swarms remains challenging, as it requires synchronization in a noisy many-body system. Here we show that cooperative transport spontaneously emerges in swarms of stochastic self-propelled agents, without requiring any form of sensing, feedback, or control. We find that a minute modification to the mechanical design of the individual agent dramatically changes its alignment response to an external force. We then show experimentally that with the proper design, a swarm of active particles spontaneously cooperates in the directional transport of larger objects. Surprisingly, transport increases with increasing payload size. A mechanical, coarse-grained description reveals that force-alignment is intrinsic and captured by a signed, charge-like parameter with units of curvature. Numerical simulations of swarms of active particles with a negative active charge corroborate the experimental findings. We analytically derive a geometrical criterion for cooperative transport which results from a bifurcation in a non-linear dynamical system. Our findings generalize existing models of active particles, offer new design rules for distributed robotic systems, and shed light on cooperation in natural swarms.","Thu, 8 Feb 2024 13:22:55 UTC (4,154 KB)"
"91","KIX: A Metacognitive Generalization Framework","Arun Kumar, Paul Schrater","Artificial Intelligence (cs.AI)","Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.","Thu, 8 Feb 2024 01:41:28 UTC (500 KB)"
"92","A computational approach to visual ecology with deep reinforcement learning","Sacha Sokoloski, Jure Majnik, Philipp Berens","Neural and Evolutionary Computing (cs.NE)","Animal vision is thought to optimize various objectives from metabolic efficiency to discrimination performance, yet its ultimate objective is to facilitate the survival of the animal within its ecological niche. However, modeling animal behavior in complex environments has been challenging. To study how environments shape and constrain visual processing, we developed a deep reinforcement learning framework in which an agent moves through a 3-d environment that it perceives through a vision model, where its only goal is to survive. Within this framework we developed a foraging task where the agent must gather food that sustains it, and avoid food that harms it. We first established that the complexity of the vision model required for survival on this task scaled with the variety and visual complexity of the food in the environment. Moreover, we showed that a recurrent network architecture was necessary to fully exploit complex vision models on the most visually demanding tasks. Finally, we showed how different network architectures learned distinct representations of the environment and task, and lead the agent to exhibit distinct behavioural strategies. In summary, this paper lays the foundation for a computational approach to visual ecology, provides extensive benchmarks for future work, and demonstrates how representations and behaviour emerge from an agent's drive for survival.","Wed, 7 Feb 2024 21:23:47 UTC (2,213 KB)"
"93","A multiscale sensorimotor model of experience-dependent behavior in a minimal organism","Maria Sol Vidal-Saez, Oscar Vilarroya, Jordi Garcia-Ojalvo","Neurons and Cognition (q-bio.NC)","To survive in ever-changing environments, living organisms need to continuously combine the ongoing external inputs they receive, representing present conditions, with their dynamical internal state, which includes influences of past experiences. It is still unclear in general, however, (i) how this happens at the molecular and cellular levels, and (ii) how the corresponding molecular and cellular processes are integrated with the behavioral responses of the organism. Here we address these issues by modeling mathematically a particular behavioral paradigm in a minimal model organism, namely chemotaxis in the nematode C. elegans. Specifically, we use a long-standing collection of elegant experiments on salt chemotaxis in this animal, in which the migration direction varies depending on its previous experience. Our model integrates the molecular, cellular and organismal levels to reproduce the experimentally observed experience-dependent behavior. The model proposes specific molecular mechanisms for the encoding of current conditions and past experiences in key neurons associated with this response, predicting the behavior of various mutants associated with those molecular circuits.","Wed, 7 Feb 2024 20:52:54 UTC (4,633 KB)"
"94","Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms","Fuda van Diggelen, Matteo De Carlo, Nicolas Cambier, Eliseo Ferrante, A.E. Eiben","Robotics (cs.RO)","Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful to address complex tasks and to survive. This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task. In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task. Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with one different neural network controller associated to each component. The whole genotype, expressing the behavior of the whole group through the two components, is subject to evolution with a single fitness function. We analyse the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism. Our experiments show three main findings: 1) The sub-groups evolve distinct emergent behaviors. 2) The effectiveness of the whole swarm depends on the interaction between the two sub-groups, leading to a more robust performance than with singular sub-group behavior. 3) The online regulatory mechanism enhances overall performance and scalability.","Wed, 7 Feb 2024 11:26:53 UTC (1,990 KB)"
"95","Review of Cetacean's click detection algorithms","Mak Gracic, Guy Gubnisky, Roee Diamant","Sound (cs.SD)","The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations. Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals. While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks' structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise. This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023. We divide the surveyed techniques into categories by their methodology. Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, supervised and unsupervised machine learning, template matching and adaptive detection approaches. Also surveyed are open access platforms for click detections, and databases openly available for testing. Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges. The paper also includes a performance comparison for several schemes over a shared database. Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications.","Wed, 7 Feb 2024 10:41:14 UTC (818 KB)"
"96","SKOOTR: A SKating, Omni-Oriented, Tripedal Robot","Adam Joshua Hung, Challen Enninful Adu, Talia Y. Moore","Robotics (cs.RO)","In both animals and robots, locomotion capabilities are determined by the physical structure of the system. The majority of legged animals and robots are bilaterally symmetric, which facilitates locomotion with consistent headings and obstacle traversal, but leads to constraints in their turning ability. On the other hand, radially symmetric animals have demonstrated rapid turning abilities enabled by their omni-directional body plans. Radially symmetric tripedal robots are able to turn instantaneously, but are commonly constrained by needing to change direction with every step, resulting in inefficient and less stable locomotion. We address these challenges by introducing a novel design for a tripedal robot that has both frictional and rolling contacts. Additionally, a freely rotating central sphere provides an added contact point so the robot can retain a stable tripod base of support while lifting and pushing with any one of its legs. The SKating, Omni-Oriented, Tripedal Robot (SKOOTR) is more versatile and stable than other existing tripedal robots. It is capable of multiple forward gaits, multiple turning maneuvers, obstacle traversal, and stair climbing. SKOOTR has been designed to facilitate customization for diverse applications: it is fully open-source, is constructed with 3D printed or off-the-shelf parts, and costs approximately $500 USD to build.","Tue, 6 Feb 2024 20:20:55 UTC (4,062 KB)"
"97","VRMM: A Volumetric Relightable Morphable Head Model","Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang","Computer Vision and Pattern Recognition (cs.CV)","In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.","Tue, 6 Feb 2024 15:55:46 UTC (40,534 KB)"
"98","Large-scale Generative AI Models Lack Visual Number Sense","Alberto Testolin, Kuinan Hou, Marco Zorzi","Computer Vision and Pattern Recognition (cs.CV)","Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with small numbers, the response variability often did not increase in a systematic way, and the pattern of errors varied with object category. Our findings demonstrate that advanced AI systems still lack a basic ability that supports an intuitive understanding of numbers, which in humans is foundational for numeracy and mathematical development.","Tue, 9 Jan 2024 18:18:32 UTC (5,495 KB)"
"99","ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds","Masato Hagiwara, Marius Miron, Jen-Yu Liu","Sound (cs.SD)","Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a ""foreign language,"" and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.","Mon, 5 Feb 2024 18:27:27 UTC (248 KB)"
"100","ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo","Computer Vision and Pattern Recognition (cs.CV)","Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.","Mon, 5 Feb 2024 05:25:33 UTC (12,463 KB)[v2] Mon, 4 Mar 2024 08:21:48 UTC (12,463 KB)"
"101","Full-body deep learning-based automated contouring of contrast-enhanced murine organs for small animal irradiator CBCT","Ethan Cramer, Sophie Dobiasch, Xinmin Liu, Stephanie E. Combs, Rodney D. Wiersma","Medical Physics (physics.med-ph)","Purpose. To alleviate the manual contouring burden, deep learning (DL) based automated contouring has been explored. However, due to the poor contrast resolution of preclinical irradiator CBCT, these methods have been limited to high contrast - minimally anatomically complex - structures such as the heart and lungs. Thus, low contrast abdominal CBCT DL-based segmentation has yet to be addressed. In this work we explore a DL-based model in conjunction with iodine-based contrast agent approach to allow precise automatic contouring of mouse abdominal, thorax, and skeletal structures in under a second.
Methods. A DL U-net-like architecture was trained to contour mice organs in small animal radiation research platform CBCT scans. 41 mice were contoured by a human expert, using semi-automatic segmentation methods, after injection of iodine contrast agent, establishing a ground truth for the DL model. The model was trained on a dataset of 26 mice, while 2 mice were used for validation, tuning the model during training, and 15 mice used for performance evaluation testing. The model consists of a pre-processor, and a post-processor for volumetric reconstruction of the DL-predicted probability maps. Model performance was evaluated using both qualitative and distance metrics, including the dice similarity score, precision score, Hausdorff Distance (HD), and mean surface distance (MSD).
Results. Performance of the DL-based iodine contrast-enhanced model provided high quality predicted contours in under a second, with the median for all organs being reported: dice $>$ 91\%, precision $>$ 95\%, HD50 $<$ 1.0 mm, and MSD $<$ 1.41 mm.
Conclusion. The proposed combination of a DL-based and iodine contrast-enhanced model proved as a viable method to vastly improve efficiency of small animal CBCT image-guided RT preclinical trials.","Sun, 4 Feb 2024 13:02:30 UTC (1,224 KB)"
"102","Brain-Body-Task Co-Adaptation can Improve Autonomous Learning and Speed of Bipedal Walking","Darío Urbina-Meléndez, Hesam Azadjou, Francisco J. Valero-Cuevas","Robotics (cs.RO)","Inspired by animals that co-adapt their brain and body to interact with the environment, we present a tendon-driven and over-actuated (i.e., n joint, n+1 actuators) bipedal robot that (i) exploits its backdrivable mechanical properties to manage body-environment interactions without explicit control, and (ii) uses a simple 3-layer neural network to learn to walk after only 2 minutes of 'natural' motor babbling (i.e., an exploration strategy that is compatible with leg and task dynamics; akin to childsplay). This brain-body collaboration first learns to produce feet cyclical movements 'in air' and, without further tuning, can produce locomotion when the biped is lowered to be in slight contact with the ground. In contrast, training with 2 minutes of 'naive' motor babbling (i.e., an exploration strategy that ignores leg task dynamics), does not produce consistent cyclical movements 'in air', and produces erratic movements and no locomotion when in slight contact with the ground. When further lowering the biped and making the desired leg trajectories reach 1cm below ground (causing the desired-vs-obtained trajectories error to be unavoidable), cyclical movements based on either natural or naive babbling presented almost equally persistent trends, and locomotion emerged with naive babbling. Therefore, we show how continual learning of walking in unforeseen circumstances can be driven by continual physical adaptation rooted in the backdrivable properties of the plant and enhanced by exploration strategies that exploit plant dynamics. Our studies also demonstrate that the bio-inspired codesign and co-adaptations of limbs and control strategies can produce locomotion without explicit control of trajectory errors.","Sun, 4 Feb 2024 07:57:52 UTC (1,697 KB)"
"103","Crash-perching on vertical poles with a hugging-wing robot","Mohammad Askari, Michele Benciolini, Hoang-Vu Phan, William Stewart, Auke J. Ijspeert, Dario Floreano","Robotics (cs.RO)","Perching with winged Unmanned Aerial Vehicles has often been solved by means of complex control or intricate appendages. Here, we present a simple yet novel method that relies on passive wing morphing for crash-landing on trees and other types of vertical poles. Inspired by the adaptability of animals' and bats' limbs in gripping and holding onto trees, we design dual-purpose wings that enable both aerial gliding and perching on poles. With an upturned nose design, the robot can passively reorient from horizontal flight to vertical upon a head-on crash with a pole, followed by hugging with its wings to perch. We characterize the performance of reorientation and perching in terms of impact speed and angle, pole material, and size. The robot robustly reorients at impact angles above 15° and speeds of 3 m/s to 9 m/s, and can hold onto various pole types larger than 28% of its wingspan in diameter. We demonstrate crash-perching on tree trunks with an overall success rate of 71%. The method opens up new possibilities for the use of aerial robots in applications such as inspection, maintenance, and biodiversity conservation.","Sat, 3 Feb 2024 09:10:04 UTC (47,306 KB)"
"104","Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents","Jiyi Wang, Likai Tang, Huimiao Chen, Sen Song","Systems and Control (eess.SY)","Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.","Fri, 2 Feb 2024 14:55:51 UTC (22,016 KB)"
"105","GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting","Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek","Computer Vision and Pattern Recognition (cs.CV)","Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS.","Fri, 2 Feb 2024 14:50:23 UTC (28,202 KB)[v2] Tue, 6 Feb 2024 16:11:35 UTC (28,202 KB)[v3] Thu, 15 Feb 2024 05:06:48 UTC (33,938 KB)"
"106","EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation","Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun","Computer Vision and Pattern Recognition (cs.CV)","Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: this https URL","Fri, 2 Feb 2024 14:04:18 UTC (28,722 KB)"
"107","RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation","Haolei Pei, Yuanyuan Xu, Yangping Zhu, Yuan Nie","Information Retrieval (cs.IR)","Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, a user shows refined multi-interests in a hierarchical structure. For example, a user likes Conan and Kuroba Kaito, which are the roles in hierarchical structure ""Animation, Japanese Animation, Detective Conan"". However, most existing methods ignore this hierarchical nature, and simply average the fine-grained interest information. Therefore, we propose a novel two-stage approach to explicitly modeling refined multi-interest in a hierarchical structure for recommendation. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in. In the second stage, the partition of retrieval space allows the EBR models to deal only with items within each circle and accurately capture users' refined interests. Experimental results show that the proposed approach achieves state-of-the-art performance. Our framework has also been deployed at Lofter.","Fri, 2 Feb 2024 09:20:48 UTC (4,782 KB)[v2] Mon, 5 Feb 2024 07:25:33 UTC (4,734 KB)[v3] Tue, 6 Feb 2024 02:24:59 UTC (4,734 KB)"
"108","Parity-time symmetry breaking enables swarming motility in Caenorhabditis elegans","Mustafa Basaran, Tevfik Can Yüce, Ali Keçebaş, Baha Altın, Yusuf Ilker Yaman, Esin Demir, Coşkun Kocabaş, Şahin K. Özdemir, Aşkın Kocabaş","Soft Condensed Matter (cond-mat.soft)","Nonreciprocal interactions break action-reaction symmetry in systems of interacting bodies. This process inevitably introduces non-Hermitian dynamics which with its hallmark signature called exceptional points (EPs) has been a subject of intense research across different disciplines ranging from photonics to metamaterials. Whether non-Hermiticity and EPs are a fundamental property of nature and if so, how nature utilizes them to gain competitive advantage have remained largely unanswered. Although biological systems feature many examples of non-reciprocal interactions with the potential to drive non-Hermitian dynamics, these are often theoretically overlooked and not experimentally investigated. Here, we demonstrate in an active matter composed of social animal Caenorhabditis elegans and bacteria, non-Hermitian dynamics, and the emergence of EPs owing to the nonreciprocal nature of oxygen sensing, nonequilibrium interfacial current, and bacterial consumption. We observed that when driven through the EP, the system collectively breaks parity-time (PT) symmetry leading to traveling waves and arrested phase separation. We further find that these features enable the collective ability to localize interfaces between broken and exact PT-phases. Remarkably, this ability provides a strong evolutionary advantage to animals living in soil. Altogether our results provide mechanistic insights into the detailed symmetries controlling the collective response of biological systems; answer a long-standing problem; and give an example of the EP-enabled dynamics in a biological system.","Thu, 1 Feb 2024 19:01:42 UTC (988 KB)"
"109","AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning","Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li","Computer Vision and Pattern Recognition (cs.CV)","Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at this https URL.","Thu, 1 Feb 2024 16:58:11 UTC (13,269 KB)"
"110","BrainSLAM: SLAM on Neural Population Activity Data","Kipp Freud, Nathan Lepora, Matt W. Jones, Cian O'Donnell","Robotics (cs.RO)","Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.","Thu, 1 Feb 2024 13:34:59 UTC (1,679 KB)"
"111","Dance-to-Music Generation with Encoder-based Textual Inversion of Diffusion Models","Sifei Li, Weiming Dong, Yuxin Zhang, Fan Tang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu","Sound (cs.SD)","The harmonious integration of music with dance movements is pivotal in vividly conveying the artistic essence of dance. This alignment also significantly elevates the immersive quality of gaming experiences and animation productions. While there has been remarkable advancement in creating high-fidelity music from textual descriptions, current methodologies mainly concentrate on modulating overarching characteristics such as genre and emotional tone. They often overlook the nuanced management of temporal rhythm, which is indispensable in crafting music for dance, since it intricately aligns the musical beats with the dancers' movements. Recognizing this gap, we propose an encoder-based textual inversion technique for augmenting text-to-music models with visual control, facilitating personalized music generation. Specifically, we develop dual-path rhythm-genre inversion to effectively integrate the rhythm and genre of a dance motion sequence into the textual space of a text-to-music model. Contrary to the classical textual inversion method, which directly updates text embeddings to reconstruct a single target object, our approach utilizes separate rhythm and genre encoders to obtain text embeddings for two pseudo-words, adapting to the varying rhythms and genres. To achieve a more accurate evaluation, we propose improved evaluation metrics for rhythm alignment. We demonstrate that our approach outperforms state-of-the-art methods across multiple evaluation metrics. Furthermore, our method seamlessly adapts to in-the-wild data and effectively integrates with the inherent text-guided generation capability of the pre-trained model. Samples are available at \url{this https URL}.","Wed, 31 Jan 2024 12:51:26 UTC (2,754 KB)"
"112","A review of statistical models used to characterize species-habitat associations with animal movement data","Katie R.N. Florko, Ron R. Togunov, Rowenna Gryba, Evan Sidrow, Steven H. Ferguson, David J. Yurkowski, Marie Auger-Méthé","Methodology (stat.ME)","Understanding species-habitat associations is fundamental to ecological sciences and for species conservation. Consequently, various statistical approaches have been designed to infer species-habitat associations. Due to their conceptual and mathematical differences, these methods can yield contrasting results. In this paper, we describe and compare commonly used statistical models that relate animal movement data to environmental data. Specifically, we examined selection functions which include resource selection function (RSF) and step-selection function (SSF), as well as hidden Markov models (HMMs) and related methods such as state-space models. We demonstrate differences in assumptions of each method while highlighting advantages and limitations. Additionally, we provide guidance on selecting the most appropriate statistical method based on research objectives and intended inference. To demonstrate the varying ecological insights derived from each statistical model, we apply them to the movement track of a single ringed seal in a case study. For example, the RSF indicated selection of areas with high prey diversity, whereas the SSFs indicated no discernable relationship with prey diversity. Furthermore, the HMM reveals variable associations with prey diversity across different behaviors. Notably, the three models identified different important areas. This case study highlights the critical significance of selecting the appropriate model to identify species-habitat relationships and specific areas of importance. Our comprehensive review provides the foundational information required for making informed decisions when choosing the most suitable statistical methods to address specific questions, such as identifying expansive corridors or protected zones, understanding movement patterns, or studying behaviours.","Tue, 30 Jan 2024 19:13:08 UTC (1,277 KB)"
"113","Democratizing the Creation of Animatable Facial Avatars","Yilin Zhu, Dalton Omens, Haodi He, Ron Fedkiw","Graphics (cs.GR)","In high-end visual effects pipelines, a customized (and expensive) light stage system is (typically) used to scan an actor in order to acquire both geometry and texture for various expressions. Aiming towards democratization, we propose a novel pipeline for obtaining geometry and texture as well as enough expression information to build a customized person-specific animation rig without using a light stage or any other high-end hardware (or manual cleanup). A key novel idea consists of warping real-world images to align with the geometry of a template avatar and subsequently projecting the warped image into the template avatar's texture; importantly, this allows us to leverage baked-in real-world lighting/texture information in order to create surrogate facial features (and bridge the domain gap) for the sake of geometry reconstruction. Not only can our method be used to obtain a neutral expression geometry and de-lit texture, but it can also be used to improve avatars after they have been imported into an animation system (noting that such imports tend to be lossy, while also hallucinating various features). Since a default animation rig will contain template expressions that do not correctly correspond to those of a particular individual, we use a Simon Says approach to capture various expressions and build a person-specific animation rig (that moves like they do). Our aforementioned warping/projection method has high enough efficacy to reconstruct geometry corresponding to each expressions.","Mon, 29 Jan 2024 20:14:40 UTC (7,589 KB)"
"114","Extending the kinematic theory of rapid movements with new primitives","Miguel A. Ferrer, Moises Diaz, Jose J. Quintana, Cristina Carmona-Duarte","Computer Vision and Pattern Recognition (cs.CV)","The Kinematic Theory of rapid movements, and its associated Sigma-Lognormal, model 2D spatiotemporal trajectories. It is constructed mainly as a temporal overlap of curves between virtual target points. Specifically, it uses an arc and a lognormal as primitives for the representation of the trajectory and velocity, respectively. This paper proposes developing this model, in what we call the Kinematic Theory Transform, which establishes a mathematical framework that allows further primitives to be used. Mainly, we evaluate Euler curves to link virtual target points and Gaussian, Beta, Gamma, Double-bounded lognormal, and Generalized Extreme Value functions to model the bell-shaped velocity profile. Using these primitives, we report reconstruction results with spatiotemporal trajectories executed by human beings, animals, and anthropomorphic robots.","Mon, 29 Jan 2024 19:45:12 UTC (4,741 KB)"
"115","Refined Inverse Rigging: A Balanced Approach to High-fidelity Blendshape Animation","Stevo Racković, Cláudia Soares, Dušan Jakovetić","Graphics (cs.GR)","In this paper, we present an advanced approach to solving the inverse rig problem in blendshape animation, using high-quality corrective blendshapes. Our algorithm introduces novel enhancements in three key areas: ensuring high data fidelity in reconstructed meshes, achieving greater sparsity in weight distributions, and facilitating smoother frame-to-frame transitions. While the incorporation of corrective terms is a known practice, our method differentiates itself by employing a unique combination of $l_1$ norm regularization for sparsity and a temporal smoothness constraint through roughness penalty, focusing on the sum of second differences in consecutive frame weights. A significant innovation in our approach is the temporal decoupling of blendshapes, which permits simultaneous optimization across entire animation sequences. This feature sets our work apart from existing methods and contributes to a more efficient and effective solution. Our algorithm exhibits a marked improvement in maintaining data fidelity and ensuring smooth frame transitions when compared to prior approaches that either lack smoothness regularization or rely solely on linear blendshape models. In addition to superior mesh resemblance and smoothness, our method offers practical benefits, including reduced computational complexity and execution time, achieved through a novel parallelization strategy using clustering methods. Our results not only advance the state of the art in terms of fidelity, sparsity, and smoothness in inverse rigging but also introduce significant efficiency improvements. The source code will be made available upon acceptance of the paper.","Mon, 29 Jan 2024 19:09:46 UTC (4,332 KB)"
"116","Computer Vision for Primate Behavior Analysis in the Wild","Richard Vogg, Timo Lüddecke, Jonathan Henrich, Sharmita Dey, Matthias Nuske, Valentin Hassler, Derek Murphy, Julia Fischer, Julia Ostner, Oliver Schülke, Peter M. Kappeler, Claudia Fichtel, Alexander Gail, Stefan Treue, Hansjörg Scherberger, Florentin Wörgötter, Alexander S. Ecker","Computer Vision and Pattern Recognition (cs.CV)","Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.","Mon, 29 Jan 2024 18:59:56 UTC (5,418 KB)"
"117","Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-free Design","Vassil Atanassov, Jiatao Ding, Jens Kober, Ioannis Havoutis, Cosimo Della Santina","Robotics (cs.RO)","Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills. However, current DRL-based frameworks usually rely on pre-existing reference trajectories obtained by capturing animal motions or transferring experience from existing controllers. This work aims to prove that learning dynamic jumping is possible without relying on imitating a reference trajectory by leveraging a curriculum design. Starting from a vertical in-place jump, we generalize the learned policy to forward and diagonal jumps and, finally, we learn to jump across obstacles. Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach yields a wide range of omnidirectional jumping motions in real-world experiments. Particularly we achieve a 90cm forward jump, exceeding all previous records for similar robots reported in the existing literature. Additionally, the robot can reliably execute continuous jumping on soft grassy grounds, which is especially remarkable as such conditions were not included in the training stage.
A supplementary video can be found on: this https URL. The code associated with this work can be found on: this https URL.","Mon, 29 Jan 2024 17:45:02 UTC (14,934 KB)[v2] Mon, 4 Mar 2024 17:28:17 UTC (15,088 KB)"
"118","Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models","Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang","Computer Vision and Pattern Recognition (cs.CV)","Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: this https URL).","Mon, 29 Jan 2024 15:21:37 UTC (19,199 KB)"
"119","Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance","Qingcheng Zhao, Pengyu Long, Qixuan Zhang, Dafei Qin, Han Liang, Longwen Zhang, Yingliang Zhang, Jingyi Yu, Lan Xu","Computer Vision and Pattern Recognition (cs.CV)","The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of lexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotional and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.","Sun, 28 Jan 2024 16:17:59 UTC (34,420 KB)[v2] Tue, 30 Jan 2024 08:23:23 UTC (8,199 KB)"
"120","An Implicit Physical Face Model Driven by Expression and Style","Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley","Computer Vision and Pattern Recognition (cs.CV)","3D facial animation is often produced by manipulating facial deformation models (or rigs), that are traditionally parameterized by expression controls. A key component that is usually overlooked is expression 'style', as in, how a particular expression is performed. Although it is common to define a semantic basis of expressions that characters can perform, most characters perform each expression in their own style. To date, style is usually entangled with the expression, and it is not possible to transfer the style of one character to another when considering facial animation. We present a new face model, based on a data-driven implicit neural physics model, that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities. Once trained, our method allows generalized physics-based facial animation for any of the trained identities, extending to unseen performances. Furthermore, it grants control over the animation style, enabling style transfer from one character to another or blending styles of different characters. Lastly, as a physics-based model, it is capable of synthesizing physical effects, such as collision handling, setting our method apart from conventional approaches.","Sat, 27 Jan 2024 14:07:48 UTC (39,133 KB)"
"121","Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting","Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang","Graphics (cs.GR)","We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \url{this https URL}.","Sat, 27 Jan 2024 06:45:22 UTC (47,237 KB)"
"122","Scattering compensation through Fourier-domain open-channel coupling in two-photon microscopy","Daniel Zepeda, Yucheng Li, Yi Xue","Optics (physics.optics)","Light penetration depth in biological tissue is limited by tissue scattering. There is an urgent need for scattering compensation in vivo focusing and imaging, particularly challenging in photon-starved scenarios, without access to the transmission side of the scattering tissue. Here, we introduce a two-photon microscopy system with Fourier-domain open-channel coupling for scattering correction (2P-FOCUS). 2P-FOCUS corrects scattering by utilizing the non-linearity of multiple-beam interference and two-photon excitation, eliminating the need for a guide star, iterative optimization, or measuring transmission or reflection matrices. We demonstrate that 2P-FOCUS significantly enhances two-photon fluorescence signals by several tens of folds when focusing through a bone sample, compared to cases without scattering compensation at equivalent excitation power. We also show that 2P-FOCUS can correct scattering over large volumes by imaging neurons and cerebral blood vessels within a 230x230x500 um\textsuperscript{3} volume in the mouse brain in vitro. 2P-FOCUS could serve as a powerful tool for deep tissue imaging in bulky organisms or live animals.","Fri, 26 Jan 2024 20:36:26 UTC (7,290 KB)"
"123","An innovative in silico model of the oral mucosa reveals the impact of extracellular spaces on chemical permeation through epithelium","Sean M. Edwards, Amy L. Harding, Joseph A. Leedale, Steve D. Webb, Helen E. Colley, Craig Murdoch, Rachel N. Bearon","Tissues and Organs (q-bio.TO)","In pharmaceutical therapeutic design or toxicology, accurately predicting the permeation of chemicals through human epithelial tissues is crucial, where permeation is significantly influenced by the tissue's cellular architecture. Current mathematical models for multi-layered epithelium such as the oral mucosa only use simplistic 'bricks and mortar' geometries and therefore do not account for the complex cellular architecture of these tissues at the microscale level, such as the extensive plasma membrane convolutions that define the extracellular spaces between cells. Chemicals often permeate tissues via this paracellular route, meaning that permeation is underestimated. To address this, measurements of human buccal mucosal tissue were conducted to ascertain the width and tortuosity of extracellular spaces across the epithelium. Using mechanistic mathematical modelling, we show that the convoluted geometry of extracellular spaces significantly impacts chemical permeation and that this can be approximated, provided that extracellular tortuosity is accounted for. We next developed an advanced physically-relevant in silico model of oral mucosal chemical permeation using partial differential equations, fitted to chemical permeation in vitro assays on tissue-engineered human oral mucosa. Tissue geometries were measured and captured in silico, and permeation examined and predicted for chemicals with different physicochemical properties. The effect of altering the extracellular space to mimic permeation enhancers was also assessed by perturbing the in silico model. This novel in vitro-in silico approach has the potential to expedite pharmaceutical innovation for testing oromucosal chemical permeation, providing a more accurate, physiologically-relevant model which can reduce animal testing with early screening based on chemical properties.","Fri, 26 Jan 2024 15:06:31 UTC (1,219 KB)"
"124","Implicit Neural Representation for Physics-driven Actuated Soft Bodies","Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran Gözcü, Markus Gross, Barbara Solenthaler","Computer Vision and Pattern Recognition (cs.CV)","Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.","Fri, 26 Jan 2024 13:42:12 UTC (35,077 KB)"
"125","Active Particle Models for Animal Behavior based on Effective Nonreciprocal Forces and Potentials","Amir Haluts, Dan Gorbonos, Nir S. Gov","Biological Physics (physics.bio-ph)","Modelling animal behavior using active-particle models is a major current challenge. The unique properties of animals mean that such models require the use of new types of effective interactions between the particles, and specifically effective forces that do not obey the usual conservation laws of Newtonian mechanics. These include nonreciprocal forces that break conservation of energy and momentum. We demonstrate here two very different animal behavior systems where such nonreciprocal effective forces naturally arise: the first is when animals form contests, as many animals do, such as fighting over some localized resource. The second system is of cohesive swarms, that are maintained by long-range adaptive attraction. These examples show that theoretical modelling in terms of active particles, interacting with effective nonreciprocal forces and potentials, expands the scope of active-particle research as well as helps to explain complex phenomena in animal behaviour.","Fri, 26 Jan 2024 13:32:16 UTC (3,785 KB)"
"126","Personality Perception in Human Videos Altered by Motion Transfer Networks","Ayda Yurtoğlu, Sinan Sonlu, Yalım Doğan, Uğur Güdükbay","Computer Vision and Pattern Recognition (cs.CV)","The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.","Fri, 26 Jan 2024 09:42:57 UTC (5,272 KB)"
"127","TIFu: Tri-directional Implicit Function for High-Fidelity 3D Character Reconstruction","Byoungsung Lim, Seong-Whan Lee","Computer Vision and Pattern Recognition (cs.CV)","Recent advances in implicit function-based approaches have shown promising results in 3D human reconstruction from a single RGB image. However, these methods are not sufficient to extend to more general cases, often generating dragged or disconnected body parts, particularly for animated characters. We argue that these limitations stem from the use of the existing point-level 3D shape representation, which lacks holistic 3D context understanding. Voxel-based reconstruction methods are more suitable for capturing the entire 3D space at once, however, these methods are not practical for high-resolution reconstructions due to their excessive memory usage. To address these challenges, we introduce Tri-directional Implicit Function (TIFu), which is a vector-level representation that increases global 3D consistencies while significantly reducing memory usage compared to voxel representations. We also introduce a new algorithm in 3D reconstruction at an arbitrary resolution by aggregating vectors along three orthogonal axes, resolving inherent problems with regressing fixed dimension of vectors. Our approach achieves state-of-the-art performances in both our self-curated character dataset and the benchmark 3D human dataset. We provide both quantitative and qualitative analyses to support our findings.","Thu, 25 Jan 2024 23:30:51 UTC (43,385 KB)"
"128","Relative Value Biases in Large Language Models","William M. Hayes, Nicolas Yax, Stefano Palminteri","Computation and Language (cs.CL)","Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.","Thu, 25 Jan 2024 21:49:32 UTC (2,607 KB)"
"129","A generic model of consciousness","Mark J. Hadley","Neurons and Cognition (q-bio.NC)","This is a model of consciousness. The hard problem of consciousness, what it feels like, is answered. The work builds on medical research analyzing the source and mechanisms associated with our feelings. It goes further by describing a generic model with wide applicability. The model is fully consistent with medical pathways in humans, but easily extends to animals and AI. The essence of the model is the interplay between associative memory and physiology. The model is a clear and concrete counterexample to the famous philosophical objections to a scientific explanation.","Tue, 2 Jan 2024 11:41:28 UTC (85 KB)"
"130","Model Predictive Wave Disturbance Rejection for Underwater Soft Robotic Manipulators","Kyle L. Walker, Cosimo Della Santina, Francesco Giorgio-Serchi","Robotics (cs.RO)","Inspired by the octopus and other animals living in water, soft robots should naturally lend themselves to underwater operations, as supported by encouraging validations in deep water scenarios. This work deals with equipping soft arms with the intelligence necessary to move precisely in wave-dominated environments, such as shallow waters where marine renewable devices are located. This scenario is substantially more challenging than calm deep water since, at low operational depths, hydrodynamic wave disturbances can represent a significant impediment. We propose a control strategy based on Nonlinear Model Predictive Control that can account for wave disturbances explicitly, optimising control actions by considering an estimate of oncoming hydrodynamic loads. The proposed strategy is validated through a set of tasks covering set-point regulation, trajectory tracking and mechanical failure compensation, all under a broad range of varying significant wave heights and peak spectral periods. The proposed control methodology displays positional error reductions as large as 84% with respect to a baseline controller, proving the effectiveness of the method. These initial findings present a first step in the development and deployment of soft manipulators for performing tasks in hazardous water environments.","Wed, 24 Jan 2024 13:16:06 UTC (2,886 KB)"
"131","GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition","Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi","Computer Vision and Pattern Recognition (cs.CV)","Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.","Wed, 24 Jan 2024 12:18:31 UTC (36,060 KB)"
"132","Common-Sense Bias Discovery and Mitigation for Classification Tasks","Miao Zhang, Zee fryer, Ben Colman, Ali Shahriyari, Gaurav Bharaj","Computer Vision and Pattern Recognition (cs.CV)","Machine learning model bias can arise from dataset composition: sensitive features correlated to the learning target disturb the model decision rule and lead to performance differences along the features. Existing de-biasing work captures prominent and delicate image features which are traceable in model latent space, like colors of digits or background of animals. However, using the latent space is not sufficient to understand all dataset feature correlations. In this work, we propose a framework to extract feature clusters in a dataset based on image descriptions, allowing us to capture both subtle and coarse features of the images. The feature co-occurrence pattern is formulated and correlation is measured, utilizing a human-in-the-loop for examination. The analyzed features and correlations are human-interpretable, so we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive correlations in a dataset, we demonstrate that downstream model bias can be mitigated by adjusting image sampling weights, without requiring a sensitive group label supervision. Experiments show that our method discovers novel biases on multiple classification tasks for two benchmark image datasets, and the intervention outperforms state-of-the-art unsupervised bias mitigation methods.","Wed, 24 Jan 2024 03:56:07 UTC (3,966 KB)[v2] Thu, 8 Feb 2024 05:38:54 UTC (3,966 KB)"
"133","Environmental impacts, nutritional profiles, and retail prices of commonly sold retail food items in 181 countries: an observational study","Elena M. Martinez, Nicole Tichenor Blackstone, Parke E. Wilde, Anna W. Herforth, William A. Masters","General Economics (econ.GN)","Affordability is often seen as a barrier to consuming sustainable diets. This study provides the first worldwide test of how retail food prices relate to empirically estimated environmental impacts and nutritional profile scores between and within food groups. We use prices for 811 retail food items commonly sold in 181 countries during 2011 and 2017, matched to estimated carbon and water footprints and nutritional profiles, to test whether healthier and more environmentally sustainable foods are more expensive between and within food groups. We find that within almost all groups, less expensive items have significantly lower carbon and water footprints. Associations are strongest for animal source foods, where each 10% lower price is associated with 20 grams lower CO2-equivalent carbon and 5 liters lower water footprint per 100kcal. Gradients between price and nutritional profile vary by food group, price range, and nutritional attribute. In contrast, lower-priced items have lower nutritional value in only some groups over some price ranges, and that relationship is sometimes reversed. These findings reveal opportunities to reduce financial and environmental costs of diets, contributing to transitions towards healthier, more environmentally sustainable food systems.","Wed, 24 Jan 2024 00:34:23 UTC (1,247 KB)"
"134","Ready for climate change? The importance of adaptive thermoregulatory flexibility for the Malagasy bat species Triaenops menamena","Sina Remmers","Quantitative Methods (q-bio.QM)","The balance between energy intake and expenditure is essential and crucial for survival for all organisms. The energy management is closely linked to the ecology. Thus, changes in environmental conditions can be challenging, especially for the animals physiology. Different strategies of thermoregulation have evolved and heterothermy seems to be the most efficient way for saving energy. Daily torpor, a temporally controlled reduction of the metabolic rate and body temperature, is one form of heterothermy and recent studies revealed that this physiological strategy is used by many tropical and subtropical species. Yet, little is known about torpor in bats and their intraspecific thermoregulatory flexibility. Therefore, three populations of the Malagasy bat species Triaenops menamena were investigated, to examine their metabolic rate, skin temperature and related energy expenditure during normothermic and torpid states in context of different microclimatic conditions. This study exposed significant physiological differences among these three populations along a gradient of fluctuation in environmental conditions. The greater the fluctuations in ambient temperature and humidity, the higher was the general resting metabolic rate and the rate of its reduction, but the lower was the torpid metabolic rate. This species shows a highly adaptive flexibility in their physiology and are able to cope with unfavorable environmental conditions by using different strategies of thermoregulation and hypometabolism, which is beneficial regarding ongoing climatic changes.","Tue, 23 Jan 2024 11:57:30 UTC (7,966 KB)"
"135","The local limit of rooted directed animals on the square lattice","Olivier Hénard, Édouard Maurel-Segala, Arvind Singh","Probability (math.PR)","We consider the local limit of finite uniformly distributed directed animals on the square lattice viewed from the root. Two constructions of the resulting uniform infinite directed animal are given: one as a heap of dominoes, constructed by letting gravity act on a right-continuous random walk and one as a Markov process, obtained by slicing the animal horizontally. We look at geometric properties of this local limit and prove, in particular, that it consists of a single vertex at infinitely many (random) levels. Several martingales are found in connection with the confinement of the infinite directed animal on the non-negative coordinates.","Tue, 23 Jan 2024 17:43:12 UTC (2,400 KB)"
"136","PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting","Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu","Graphics (cs.GR)","Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency. Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a resolution of 512 $\times$ 512 ).","Tue, 23 Jan 2024 16:40:47 UTC (6,690 KB)[v2] Wed, 24 Jan 2024 10:00:22 UTC (6,690 KB)[v3] Mon, 29 Jan 2024 08:55:57 UTC (6,690 KB)[v4] Tue, 30 Jan 2024 01:38:29 UTC (6,690 KB)"
"137","Experience-Learning Inspired Two-Step Reward Method for Efficient Legged Locomotion Learning Towards Natural and Robust Gaits","Yinghui Li, Jinze Wu, Xin Liu, Weizhong Guo, Yufei Xue","Robotics (cs.RO)","Multi-legged robots offer enhanced stability in complex terrains, yet autonomously learning natural and robust motions in such environments remains challenging. Drawing inspiration from animals' progressive learning patterns, from simple to complex tasks, we introduce a universal two-stage learning framework with two-step reward setting based on self-acquired experience, which efficiently enables legged robots to incrementally learn natural and robust movements. In the first stage, robots learn through gait-related rewards to track velocity on flat terrain, acquiring natural, robust movements and generating effective motion experience data. In the second stage, mirroring animal learning from existing experiences, robots learn to navigate challenging terrains with natural and robust movements using adversarial imitation learning. To demonstrate our method's efficacy, we trained both quadruped robots and a hexapod robot, and the policy were successfully transferred to a physical quadruped robot GO1, which exhibited natural gait patterns and remarkable robustness in various terrains.","Mon, 22 Jan 2024 22:34:57 UTC (31,780 KB)"
"138","Emergent Dominance Hierarchies in Reinforcement Learning Agents","Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky","Multiagent Systems (cs.MA)","Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.
In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: dominance hierarchies.
We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a similar structure to those studied in chickens, mice, fish, and other species.","Sun, 21 Jan 2024 16:59:45 UTC (1,401 KB)[v2] Thu, 1 Feb 2024 16:50:23 UTC (1,401 KB)[v3] Tue, 13 Feb 2024 12:32:35 UTC (1,402 KB)[v4] Tue, 27 Feb 2024 19:36:43 UTC (1,402 KB)"
"139","CloSe: A 3D Clothing Segmentation Dataset and Model","Dimitrije Antić, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, Gerard Pons-Moll","Computer Vision and Pattern Recognition (cs.CV)","3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at this https URL.","Mon, 22 Jan 2024 15:42:21 UTC (19,711 KB)"
"140","Lightweight Self-Driven Deformable Organ Animations","Benjamnin Kenwright, Kanida Sinmai","Graphics (cs.GR)","The subject of simulating internal organs is a valuable and important topic of research to multiple fields from medical analysis to education and training. This paper presents a solution that utilizes a graphical technique in combination with a Stochastic method for tuning an active physics-based model. We generate responsive interactive organ animations with regional properties (i.e., areas of the model oscillating with different harmonic frequencies) to reproduce and capture real-world characteristics. Our method builds upon biological and physical discoveries to procedurally generate internally controlled rhythmic motions but also enable the solution to be interactive and adaptive. We briefly review deformation models for medical simulations and investigate the impediments to combining 'computergraphics' representations with biomechanical models. Finally, we present a lightweight solution that is scalable and able to procedurally generate large organ animations. In particular, simplified geometric representations of deformable structures that use periodic coupled forces to drive themselves.","Sun, 21 Jan 2024 22:37:15 UTC (3,898 KB)"
"141","A Comprehensive Approach to Characterize Navigation Instruments for Magnetic Guidance in Biological Systems","Peter Blümler, Fabian Raudzus, Friederike Schmid","Biological Physics (physics.bio-ph)","The non-invasive spatiotemporal control of cellular functions, organization of tissues, and even the behavior of small animals has become paramount for advanced therapies. As magnetic fields do not interact with biological matter, their application is not only suitable for in vitro experiments but also for in vivo applications, even in deep tissues. Particularly, the remote manipulation of paramagnetic entities through magnetic instruments has emerged as a promising approach across various biological contexts. Despite similarities in basic experimental concepts, variations in the properties and descriptions of those magnetic instruments among the authors and studies resulted in a lack of reproducibility and comparability. Therefore, this article addresses the question of how to standardize the characterization of magnetic instruments. Our emphasis lies on the ability of magnetic systems to control the movement of paramagnetic objects such as ferro- or superparamagnetic particles, within organisms. This movement is achieved by exerting a force on magnetic particles by exposing them to a locally varying magnetic field. While it is well-known that the exerted force depends on the spatial variation (i.e. the gradient) of the magnetic field, the magnitude of the field is equally important. However, this second factor is often neglected in the literature. Therefore, we conduct a comprehensive analysis and discussion of both factors. Furthermore, we propose a novel descriptor, termed ""effective gradient"", which combines both dependencies. To illustrate, we characterize different magnet systems by calculating and comparing the different quantities and relating them to two experiments with different superparamagnetic nanoparticles.","Fri, 19 Jan 2024 22:50:36 UTC (2,258 KB)"
"142","Homodyned K-Distribution Parameter Estimation in Quantitative Ultrasound: Autoencoder and Bayesian Neural Network Approaches","Ali K. Z. Tehrani, Guy Cloutier, An Tang, Ivan M. Rosado-Mendez, Hassan Rivaz","Signal Processing (eess.SP)","Quantitative ultrasound (QUS) analyzes the ultrasound backscattered data to find the properties of scatterers that correlate with the tissue microstructure. Statistics of the envelope of the backscattered radiofrequency (RF) data can be utilized to estimate several QUS parameters. Different distributions have been proposed to model envelope data. The homodyned K-distribution (HK distribution) is one of the most comprehensive distributions that can model ultrasound backscattered envelope data under diverse scattering conditions (varying scatterer number density and coherent scattering). The scatterer clustering parameter (alpha) and the ratio of the coherent to diffuse scattering power (k) are the parameters of this distribution that have been used extensively for tissue characterization in diagnostic ultrasound. The estimation of these two parameters (which we refer to as HK parameters) is done using optimization algorithms in which statistical features such as the envelope point-wise signalto-noise ratio (SNR), skewness, kurtosis, and the log-based moments have been utilized as input to such algorithms. The optimization methods minimize the difference between features and their theoretical value from the HK model. We propose that the true value of these statistical features is a hyperplane that covers a small portion of the feature space. In this paper, we follow two approaches to reduce the effect of sample features' error. We propose a model projection neural network based on denoising autoencoders to project the noisy features into this space based on this assumption. We also investigate if the noise distribution can be learned by the deep estimators. We compare the proposed methods with conventional methods using simulations, an experimental phantom, and data from an in vivo animal model of hepatic steatosis. A demo code are available online at this http URL","Fri, 19 Jan 2024 19:59:16 UTC (4,706 KB)"
"143","Fast Registration of Photorealistic Avatars for VR Facial Animation","Chaitanya Patel, Shaojie Bai, Te-Li Wang, Jason Saragih, Shih-En Wei","Computer Vision and Pattern Recognition (cs.CV)","Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided image-to-image style transfer module that is conditioned on current estimation of expression and head pose. These two modules reinforce each other, as image style transfer becomes easier when close-to-ground-truth examples are shown, and better domain-gap removal helps registration. Our system produces high-quality results efficiently, obviating the need for costly offline registration to generate personalized labels. We validate the accuracy and efficiency of our approach through extensive experiments on a commodity headset, demonstrating significant improvements over direct regression methods as well as offline registration.","Fri, 19 Jan 2024 19:42:38 UTC (37,036 KB)"
"144","Synthesizing Moving People with 3D Control","Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik","Computer Vision and Pattern Recognition (cs.CV)","In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: this https URL.","Fri, 19 Jan 2024 18:59:11 UTC (5,760 KB)"
"145","In vivo exposure of the bladder using a non-invasive high intensity focused ultrasound toroidal transducer","Victor Delattre (LabTAU), Sophie Cambronero (LabTAU), Yao Chen (LabTAU), Gail Ter Haar, Ian Rivens, Gerry Polton, Cyril Lafon (LabTAU), David Melodelima (LabTAU)","Medical Physics (physics.med-ph)","A toroidal high-intensity focused ultrasound (HIFU) transducer was used to expose normal bladder wall tissues non-invasively in vivo in a porcine model in order to investigate the potential to treat bladder tumors. The transducer was divided into 32 concentric rings with equal surface areas, operating at 2.5 MHz. Eight animals were split into two groups of 4. In the first group, post-mortem evaluation was performed immediately after ultrasound exposure. In the second group, animals survived for up to seven days before post-mortem evaluation. The ultrasound imaging guided HIFU device was hand-held during the procedure using optical tracking to ensure correct targeting. One thermal lesion in each animal was created using a 40 s exposure at 80 acoustic Watts (free-field) in the trigone region of the bladder wall. The average ($\pm$Standard Deviation) abdominal wall and bladder wall thicknesses were 10.3 $\pm$ 1.4 mm and 1.1 $\pm$ 0.4 mm respectively. The longest and shortest axes of the HIFU ablations were 7.7 $\pm$ 2.9 mm and 6.0 $\pm$ 1.8 mm, respectively, resulting in an ablation of the whole thickness of the bladder wall in most cases. Ablation were performed at an average depth (distance from the skin surface to the centre of the HIFU lesion) of 42.5 $\pm$ 3.8 mm and extended throughout the thickness of the bladder. There were two cases of injury to tissues immediately adjacent to the bladder wall but without signs of perforation, as confirmed by histological analysis. Non-invasive HIFU ablation using a hand-held toroidal transducer was successfully performed to destroy regions of the bladder wall in vivo.","Fri, 19 Jan 2024 09:36:25 UTC (548 KB)"
"146","Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry","Yu Han, Jingwen Tao","Computers and Society (cs.CY)","This document offers a critical overview of the emerging trends and significant advancements in artificial intelligence (AI) within the pharmaceutical industry. Detailing its application across key operational areas, including research and development, animal testing, clinical trials, hospital clinical stages, production, regulatory affairs, quality control and other supporting areas, the paper categorically examines AI's role in each sector. Special emphasis is placed on cutting-edge AI technologies like machine learning algorithms and their contributions to various aspects of pharmaceutical operations. Through this comprehensive analysis, the paper highlights the transformative potential of AI in reshaping the pharmaceutical industry's future.","Fri, 5 Jan 2024 04:01:09 UTC (1,014 KB)[v2] Mon, 22 Jan 2024 04:47:20 UTC (1,011 KB)"
"147","Solving the $106$ years old $3^k$ points problem with the clockwise-algorithm","Marco Ripà","General Mathematics (math.GM)","In this paper, we present the clockwise-algorithm that solves the extension in $k$-dimensions of the infamous nine-dot problem, the well-known two-dimensional thinking outside the box puzzle. We describe a general strategy that constructively produces minimum length covering trails, for any $k \in \mathbb{N}-\{0\}$, solving the NP-complete $(3 \times 3 \times \cdots \times 3)$-point problem inside $3 \times 3 \times \cdots \times 3$ hypercubes. In particular, using our algorithm, we explicitly draw different covering trails of minimal length $h(k)=\frac{3^k-1}{2}$, for $k=3, 4, 5$. Furthermore, we conjecture that, for every $k \geq 1$, it is possible to solve the $3^k$-point problem with $h(k)$ lines starting from any of the $3^k$ nodes, except from the central one. Finally, we cover a $3 \times 3 \times 3$ grid with a tree of size $12$.","Sun, 14 Jan 2024 16:51:21 UTC (1,909 KB)"
"148","A method for characterizing disease emergence curves from paired pathogen detection and serology data","Joshua Hewitt, Grete Wilson-Henjum, Derek T. Collins, Jourdan M. Ringenberg, Christopher A. Quintanal, Robert Pleszewski, Jeffrey C. Chandler, Thomas J. DeLiberto, Kim M. Pepin","Methodology (stat.ME)","Wildlife disease surveillance programs and research studies track infection and identify risk factors for wild populations, humans, and agriculture. Often, several types of samples are collected from individuals to provide more complete information about an animal's infection history. Methods that jointly analyze multiple data streams to study disease emergence and drivers of infection via epidemiological process models remain underdeveloped. Joint-analysis methods can more thoroughly analyze all available data, more precisely quantifying epidemic processes, outbreak status, and risks. We contribute a paired data modeling approach that analyzes multiple samples from individuals. We use ""characterization maps"" to link paired data to epidemiological processes through a hierarchical statistical observation model. Our approach can provide both Bayesian and frequentist estimates of epidemiological parameters and state. We motivate our approach through the need to use paired pathogen and antibody detection tests to estimate parameters and infection trajectories for the widely applicable susceptible, infectious, recovered (SIR) model. We contribute general formulas to link characterization maps to arbitrary process models and datasets and an extended SIR model that better accommodates paired data. We find via simulation that paired data can more efficiently estimate SIR parameters than unpaired data, requiring samples from 5-10 times fewer individuals. We then study SARS-CoV-2 in wild White-tailed deer (Odocoileus virginianus) from three counties in the United States. Estimates for average infectious times corroborate captive animal studies. Our methods use general statistical theory to let applications extend beyond the SIR model we consider, and to more complicated examples of paired data.","Thu, 18 Jan 2024 15:24:56 UTC (1,404 KB)"
"149","Continuous Piecewise-Affine Based Motion Model for Image Animation","Hexiang Wang, Fengqi Liu, Qianyu Zhou, Ran Yi, Xin Tan, Lizhuang Ma","Computer Vision and Pattern Recognition (cs.CV)","Image animation aims to bring static images to life according to driving videos and create engaging visual content that can be used for various purposes such as animation, entertainment, and education. Recent unsupervised methods utilize affine and thin-plate spline transformations based on keypoints to transfer the motion in driving frames to the source image. However, limited by the expressive power of the transformations used, these methods always produce poor results when the gap between the motion in the driving frame and the source image is large. To address this issue, we propose to model motion from the source image to the driving frame in highly-expressive diffeomorphism spaces. Firstly, we introduce Continuous Piecewise-Affine based (CPAB) transformation to model the motion and present a well-designed inference algorithm to generate CPAB transformation from control keypoints. Secondly, we propose a SAM-guided keypoint semantic loss to further constrain the keypoint extraction process and improve the semantic consistency between the corresponding keypoints on the source and driving images. Finally, we design a structure alignment loss to align the structure-related features extracted from driving and generated images, thus helping the generator generate results that are more consistent with the driving action. Extensive experiments on four datasets demonstrate the effectiveness of our method against state-of-the-art competitors quantitatively and qualitatively. Code will be publicly available at: this https URL.","Wed, 17 Jan 2024 11:40:05 UTC (1,695 KB)"
"150","Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent","Hugo Laurencon, Yesoda Bhargava, Riddhi Zantye, Charbel-Raphaël Ségerie, Johann Lussange, Veeky Baths, Boris Gutkin","Artificial Intelligence (cs.AI)","Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dynamically choose policies that favor homeostasis in a continuously changing internal-state milieu. Results of our experiments demonstrate that agent learns homeostatic behaviour in a CTCS environment, making CTCS-HRRL a promising framework for modellng animal dynamics and decision-making.","Wed, 17 Jan 2024 06:29:34 UTC (1,898 KB)"
